%%
%% This is file `sample-sigconf-authordraft.tex',
%% generated with the docstrip utility.
%%
%% The original source files were:
%%
%% samples.dtx  (with options: `all,proceedings,bibtex,authordraft')
%% 
%% IMPORTANT NOTICE:
%% 
%% For the copyright see the source file.
%% 
%% Any modified versions of this file must be renamed
%% with new filenames distinct from sample-sigconf-authordraft.tex.
%% 
%% For distribution of the original source see the terms
%% for copying and modification in the file samples.dtx.
%% 
%% This generated file may be distributed as long as the
%% original source files, as listed above, are part of the
%% same distribution. (The sources need not necessarily be
%% in the same archive or directory.)
%%
%%
%% Commands for TeXCount
%TC:macro \cite [option:text,text]
%TC:macro \citep [option:text,text]
%TC:macro \citet [option:text,text]
%TC:envir table 0 1
%TC:envir table* 0 1
%TC:envir tabular [ignore] word
%TC:envir displaymath 0 word
%TC:envir math 0 word
%TC:envir comment 0 0
%%
%% The first command in your LaTeX source must be the \documentclass
%% command.
%%
%% For submission and review of your manuscript please change the
%% command to \documentclass[manuscript, screen, review]{acmart}.
%%
%% When submitting camera ready or to TAPS, please change the command
%% to \documentclass[sigconf]{acmart} or whichever template is required
%% for your publication.
%%
%%
\documentclass[acmsmall,screen,review,anonymous]{acmart} % FSE review version
%%
%% \BibTeX command to typeset BibTeX logo in the docs
\AtBeginDocument{%
  \providecommand\BibTeX{{%
    Bib\TeX}}}

%% Rights management information.  This information is sent to you
%% when you complete the rights form.  These commands have SAMPLE
%% values in them; it is your responsibility as an author to replace
%% the commands and values with those provided to you when you
%% complete the rights form.
\setcopyright{acmlicensed}
\copyrightyear{2026}
\acmYear{2026}
\acmDOI{XXXXXXX.XXXXXXX}
%% These commands are for a PROCEEDINGS abstract or paper.
\acmConference[Conference ICSE '2026]{IEEE/ACM International Conference on Software Engineering}{April 12--18,
  2026}{Rio de Janeiro, Brazil}
%%
%%  Uncomment \acmBooktitle if the title of the proceedings is different
%%  from ``Proceedings of ...''!
%%
%%\acmBooktitle{Woodstock '18: ACM Symposium on Neural Gaze Detection,
%%  June 03--05, 2018, Woodstock, NY}
\acmISBN{978-1-4503-XXXX-X/2018/06}


\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmic}
% \usepackage{algorithmicx}
% \usepackage{algpseudocode}
% \usepackage{algorithm2e}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

% \algnewcommand\algorithmicforeach{\textbf{for each}}
% \algdef{S}[FOR]{ForEach}[1]{\algorithmicforeach\ #1\ \algorithmicdo}

%%
%% Submission ID.
%% Use this when submitting an article to a sponsored event. You'll
%% receive a unique submission ID from the organizers
%% of the event, and this ID should be used as the parameter to this command.
%%\acmSubmissionID{123-A56-BU3}

%%
%% For managing citations, it is recommended to use bibliography
%% files in BibTeX format.
%%
%% You can then either use BibTeX with the ACM-Reference-Format style,
%% or BibLaTeX with the acmnumeric or acmauthoryear sytles, that include
%% support for advanced citation of software artefact from the
%% biblatex-software package, also separately available on CTAN.
%%
%% Look at the sample-*-biblatex.tex files for templates showcasing
%% the biblatex styles.
%%

%%
%% The majority of ACM publications use numbered citations and
%% references.  The command \citestyle{authoryear} switches to the
%% "author year" style.
%%
%% If you are preparing content for an event
%% sponsored by ACM SIGGRAPH, you must use the "author year" style of
%% citations and references.
%% Uncommenting
%% the next command will enable that style.
%%\citestyle{acmauthoryear}


%%
%% end of the preamble, start of the body of the document source.
\begin{document}

\newcommand{\circlenum}[1]{\normalsize{\textcircled{\scriptsize{\emph{#1}}}}\normalsize}

%%
%% The "title" command has an optional parameter,
%% allowing the author to define a "short title" to be used in page headers.
\title{Toward Semantic Understanding of Stripped Binary Android Native Libraries: A framework Based on Multi-Agent System}

%%
%% The "author" command and its associated commands are used to define
%% the authors and their affiliations.
%% Of note is the shared affiliation of the first two authors, and the
%% "authornote" and "authornotemark" commands
%% used to denote shared contribution to the research.
\author{Ben Trovato}
\authornote{Both authors contributed equally to this research.}
\email{trovato@corporation.com}
\orcid{1234-5678-9012}
\author{G.K.M. Tobin}
\authornotemark[1]
\email{webmaster@marysville-ohio.com}
\affiliation{%
  \institution{Institute for Clarity in Documentation}
  \city{Dublin}
  \state{Ohio}
  \country{USA}
}

\author{Lars Th{\o}rv{\"a}ld}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{larst@affiliation.org}

\author{Valerie B\'eranger}
\affiliation{%
  \institution{Inria Paris-Rocquencourt}
  \city{Rocquencourt}
  \country{France}
}

\author{Aparna Patel}
\affiliation{%
 \institution{Rajiv Gandhi University}
 \city{Doimukh}
 \state{Arunachal Pradesh}
 \country{India}}

\author{Huifen Chan}
\affiliation{%
  \institution{Tsinghua University}
  \city{Haidian Qu}
  \state{Beijing Shi}
  \country{China}}

\author{Charles Palmer}
\affiliation{%
  \institution{Palmer Research Laboratories}
  \city{San Antonio}
  \state{Texas}
  \country{USA}}
\email{cpalmer@prl.com}

\author{John Smith}
\affiliation{%
  \institution{The Th{\o}rv{\"a}ld Group}
  \city{Hekla}
  \country{Iceland}}
\email{jsmith@affiliation.org}

\author{Julius P. Kumquat}
\affiliation{%
  \institution{The Kumquat Consortium}
  \city{New York}
  \country{USA}}
\email{jpkumquat@consortium.net}

%%
%% By default, the full list of authors will be used in the page
%% headers. Often, this list is too long, and will overlap
%% other information printed in the page headers. This command allows
%% the author to define a more concise list
%% of authors' names for this purpose.
\renewcommand{\shortauthors}{Trovato et al.}

%%
%% The abstract is a short summary of the work to be presented in the
%% article.
\begin{abstract}
  An increasing number of Android applications encapsulate parts of their logic in native code to improve execution efficiency or to impede reverse engineering. Ultimately, the native code is compiled into binaries, specifically shared objects designated by the .so suffix. Despite extensive research on stripped binaries, there remains a significant gap in the detailed exploration of those on the ARM64 architecture, particularly for Android Native Libraries. This paper proposes a framework based on a Multi-Agent System that models the manual reverse engineering process. This framework integrates three key components: Ghidra for static analysis (providing assembly and pseudo-C code), Frida Stalker for dynamic analysis (simulating debugging behaviors), and a Large Language Model (LLM) for automated test case generation. To evaluate our method's effectiveness, we constructed a new dataset of stripped ARM64 binaries by collecting open-source Android applications with native libraries from GitHub. Extensive experiments demonstrate that our method has achieved state-of-the-art (SOTA) performance in semantic understanding tasks, including function name recovery and comment generation.
\end{abstract}

%%
%% The code below is generated by the tool at http://dl.acm.org/ccs.cfm.
%% Please copy and paste the code instead of the example below.
%%
\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>00000000.0000000.0000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>00000000.00000000.00000000</concept_id>
  <concept_desc>Do Not Use This Code, Generate the Correct Terms for Your Paper</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}



%%
%% Keywords. The author(s) should pick words that accurately describe
%% the work being presented. Separate the keywords with commas.
\keywords{Do, Not, Us, This, Code, Put, the, Correct, Terms, for,
  Your, Paper}
%% A "teaser" image appears between the author and affiliation
%% information and the body of the document, and typically spans the
%% page.
% \begin{teaserfigure}
%   \includegraphics[width=\textwidth]{sampleteaser}
%   \caption{Seattle Mariners at Spring Training, 2010.}
%   \Description{Enjoying the baseball game from the third-base
%   seats. Ichiro Suzuki preparing to bat.}
%   \label{fig:teaser}
% \end{teaserfigure}

\received{20 February 2007}
\received[revised]{12 March 2009}
\received[accepted]{5 June 2009}

%%
%% This command processes the author and affiliation and title
%% information and builds the first part of the formatted document.
\maketitle


\section{Introduction}



\section{Background and Motivation}



\subsection{Challenges}
Although the task of recovery of binary function names has garnered significant attention, leading to a variety of methods that leverage different information in different ways and have each achieved a certain degree of success, we have identified three persistent challenges in this domain.

\textbf{C1: Generalizability of Function Name Recovery}. Previous research has predominantly framed binary function name recovery as a classification task (e.g., XFL, AsmDepictor, SymLM). This approach presupposes that the target function names, or at least parts of them, exist within their predefined set of classes. However, such an assumption often does not hold in real-world scenarios. Although various techniques have been employed to mitigate the Out-of-Vocabulary (OOV) problem, it remains an unavoidable issue, particularly when these methods are applied to novel datasets. The recent state-of-the-art technique, SymGen, reformulates the task as a generation problem, fundamentally resolving the OOV issue by fine-tuning a Large Language Model (LLM). Nevertheless, due to resource and dataset constraints, such generative approaches typically resort to fine-tuning smaller models (e.g., CodeLlama-34B). This reliance on smaller models significantly curtails their effectiveness when the target function or its other architectural variants are not present in the fine-tuning data. The core reason is the limited capacity of smaller models to truly comprehend the semantics of binary code, which hinders their ability to generalize to unseen functions. Furthermore, the ever-increasing volume of binary code in modern software poses an additional threat to the generalizability of the strategy of fine-tuning LLM.

\textbf{C2: Pervasive Function Calls}. Complex function call graphs are an ubiquitous feature of code written by developers. Figure xxx shows the function \lstinline{copynode234}, which employs recursion to traverse a 2-3-4 tree while delegating the node copying task to the \lstinline{copyfn} function. Although some small functions may be inlined by the compiler, the vast majority of intricate call relationships are preserved in binaries. At the source code level, one can often infer the semantics of a function by examining the names of the functions it calls. In stripped binaries, however, this semantic information is largely lost, leaving us with only the starting addresses of callee functions (e.g., \lstinline{FUN_00123456}), which substantially complicates semantic comprehension. A more formidable challenge arises from the widespread use of indirect calls in C/C++ (e.g., function pointers, virtual function tables), which makes it impossible to resolve call targets through static analysis alone. \lstinline{FUN_00125a60} in Figure xxx is the decompiled counterpart of copynode234. The call to the copyfn function corresponds to \lstinline{(*param_2)(param_3)} on line 5. Since the target of this call depends on the value of \lstinline{param_2}, its actual destination cannot be determined by purely static analysis. This ambiguity obstructs the understanding of the callee's semantics, impeding the recovery of \lstinline{FUN_00125a60}.

Taking SymGen as an example, we observed that many of its predicted names were placeholders like \lstinline{FUN_xxxxxx}. A closer analysis revealed that this often occurs when the primary logic of a target function A is implemented by calling another function B. Because B's name has been stripped and replaced with a placeholder, the predicted name for A inherits this placeholder. While some methods like SymLM and NERO have attempted to model call context information, their performance has been suboptimal. We argue that this is primarily because they fail to account for the topological dependency in the call graph; that is, the logic of copynode234 can be accurately inferred only after the logic of copyfn has been determined.

\textbf{C3: Unpredictable Execution Behavior}. We contend that static code analysis alone is insufficient for a complete understanding of program logic, especially when the code involves dynamic behaviors influenced by runtime parameters or external environment variables. This holds true even for source-level code comprehension, although the rich semantic information available in source code often mitigates its impact. In the context of stripped binaries, the effect of dynamic behavior becomes far more pronounced. This challenge is further exacerbated by modern compilers, which extensively employ optimization techniques that obscure the code's true behavior, making it exceedingly difficult to grasp its logic from a static view.

Consider a function \lstinline{ParseNanos} from a real-world project (RTranslator) and its decompiled variant, shown in Figures xxx and xxx, respectively. The function accepts string pointer data, writes the parsed nanosecond value to the output parameter \lstinline{nanos}, and returns the new position of the data pointer. Its core logic, a loop spanning lines 4-8, extracts digits from the string and stores them in value. Lines 9-12 pad the number with zeros if it has fewer than 9 digits, and the final result is output on lines 13 and 14. The source code is readily understandable. However, after compilation with optimizations (-O2), its decompiled representation exhibits altered logical expressions, although the functionality remains equivalent. For example, the original condition "parse at most 9 characters" is transformed into \lstinline{if(8 < uVar3)}. As shown in Table xxx, this transformation not only increases the difficulty of semantic understanding but can also mislead predictions of models.

\subsection{Motivation}

In recent years, LLMs have demonstrated remarkable efficacy across numerous source-level programming tasks. Pioneering research such as SymGen and FoC has illuminated their potential for understanding the semantics of binary code. We are therefore convinced that leveraging capacities of LLMs remains a promising strategy. To address challenge \textbf{C1}, instead of fine-tuning small models like CodeLlama, our approach, HyBinMAS, utilizes a larger, general-purpose LLM as its foundational component, structured within a Multi-Agent System. It eliminates the need for meticulous fine-tuning, thereby attaining superior generalizability, considering the inherent drawbacks of fine-tuning large-scale LLMs, which is both resource-intensive and time-consuming.

To tackle \textbf{C2}, HyBinMAS begins by preprocessing the target binary file, employing a hybrid static-dynamic analysis to resolve function call graphs. It then explicitly considers these call relationships during the name prediction phase. We observed that reverse engineers often adopt a recursive mindset when analyzing function calls: if function A calls function B, they typically analyze B first and use the resulting semantic understanding to inform their analysis of A. Inspired by this practice, HyBinMAS follows a similar bottom-up process. It first recovers the names of the functions called within a target function and then leverages this information to recover the name of the target function itself. Returning to the example in Figure xxx, by analyzing the call relationships of \lstinline{FUN_00125a60}, we can determine the actual function pointed to by \lstinline{param_2}, deduce that its logic is related to copying, and consequently predict a more accurate name, as shown in Figure xxx. Without this strategy, one might only discern that the function performs a recursive traversal, without knowing the specific operation being performed during the traversal.

 An experienced reverse engineer not only "reads" the code but also "runs" it. After forming an initial hypothesis about the code's semantics through static inspection, they will construct test cases to execute the function and observe its dynamic behavior. This allows them to validate or refute their initial assumptions and guide further analysis. We believe this is a highly effective method for addressing the challenge of \textbf{C3}. For the function \lstinline{ParseNanos}, the initial LLM predictions in Table xxx were semantically close but incorrect regarding the precise number of digits, which is a fitting example of being misled by compilation optimizations. Based on the initial semantic hypothesis, HyBinMAS generates the test cases shown in Table xxx. Upon execution, it observes that for test case 4, which expects only the first 8 characters to be parsed, all 9 digits are actually processed. This execution trace is summarized into a feedback report that directs the LLM to revise its previous inference. Through several iterations of this process, HyBinMAS arrives at the more accurate semantic prediction shown in the HyBinMAS column of Table xxx.

Furthermore, we acknowledge that not all functions can be executed successfully. A more complex example is presented in Figure xxx (see Appendix for decompiled code). The LLM's initial semantic interpretation was simply \lstinline{remove}, a description that is clearly imprecise. However, because this function takes a pointer to a custom structure as a parameter, attempts to generate test cases and execute it frequently lead to illegal memory access and program crashes. HyBinMAS addresses this by using an additional LLM to summarize the generated test cases and the execution failure logs into a new feedback report. By providing this consolidated feedback to the primary LLM, HyBinMAS facilitates a more accurate prediction: \lstinline{remove_env}.


% ### Challenges

% 虽然二进制函数名恢复任务广受关注，在该领域涌现了许多相关的方法（综述中提到的方法），它们采用不同方式利用不同的信息并分别取得了一定程度的进展。但是我们发现二进制函数名称恢复任务仍然存在如下3个挑战：

% #### C1 方法的泛用性

% 以往的研究都将二进制函数名称恢复任务建模为一个分类任务（XFL、AsmDepictor、SymLM），这意味着它们的方法建立在目标函数名（部分）存在于它们的类别集合中。但是这样的假设往往很难在现实的函数名称恢复任务中成立，虽然它们采用了不同方式缓解OOV的问题，但OOV问题仍然不可避免，特别是当被用到新数据集中时。最近的SOTA（SymGen）将函数名称恢复任务建模为一个生成任务并通过微调一个LLM来处理，并从根本上解决了OOV的问题。但是由于资源和数据集的限制，这一类方法通常选择参数量较小的模型（例如CodeLLama-34B）进行微调，这使得被预测的函数或其指定架构的变体不在微调数据集中时，方法的有效性大打折扣，这主要是因为小参数模型难以真正理解二进制代码的知识，因而难以完全将其推广到未见过的二进制函数中。而且如今日益增多的二进制代码也使得微调LLM的策略的泛用性受到一定程度的影响。

% #### C2 广泛存在的函数调用

% 现实中的程序员编写的代码中广泛存在复杂的函数调用情况，如图xxx所示，函数copynode234内部通过调用copyfn完成实际的节点复制功能，并通过递归调用自身完成遍历2-3-4树的操作。虽然在将源码编译成二进制代码时，有些短小的函数会发生函数内联，但是绝大多数的复杂的调用关系仍然会被保留下来。对于源码级的函数语义理解任务，可以通过观察被调用函数的函数名来推测其语义，但在stripped binaries中，我们丢失了绝大多数的语义信息，我们只能知道被调用函数的起始地址（例如FUN_00123456），这无疑加剧了函数理解的困难。更困难的是，C/C++代码中广泛存在间接调用的行为（例如函数指针、虚函数表等）使得我们无法在静态分析中获取其调用目标。例如图xxx表示了copynode234在编译成二进制代码后的反编译对等体，第5行的`(*param_2)(param_3)`对应了`copyfn`函数的调用，它的值取决于param_2，因而无法在纯静态分析中知道其实际指向的函数，更无法知道该函数的语义信息，因而在恢复FUN_00125a60的函数名（copynode234）时存在困难。

% 以SymGen为例，我们发现它的预测结果中存在许多`FUN_xxxxxx`，仔细分析后发现它们主要是因为目标函数A的内部的主要逻辑是通过调用另一个函数B完成的，而由于B的函数名被strip掉而被替换为`FUN_xxxxxx`，因此最终的预测名也就是这样的占位符。虽然有些方法（SymLM、NERO）考虑到了将调用上下文信息进行建模，但效果并不理想。我们认为这主要是因为他们并没有考虑这种拓扑关系，即只有确定了copyfn的逻辑，才能更准确地推测copynode234的逻辑。

% #### C3 难以预测的执行行为

% 我们强调仅通过静态代码难以完全理解代码的逻辑，特别是当代码涉及动态行为时（例如代码的逻辑收到参数、外部环境变量的影响）。这样的事实对于源码级代码理解任务同样适用，只不过源码中丰富的语义信息削弱了它的影响。但当场景转化为stripped binaries时，动态行为的影响便凸显出来了，尤其是现代编译器在将源代码编译成二进制代码时广泛使用编译优化技术，更进一步隐藏了代码的真实行为，使得我们难以仅通过静态代码准确理解代码的逻辑。

% 以图xxx和图xxx为例，它们是一个真实项目（RTranslator）中的某个函数ParseNanos的源码和对应的反编译变体。它接受一个表示字符串的指针data，并将第二个参数nanos作为解析结果输出，并返回解析后data指向的新位置。它的核心逻辑在于第4-8行的while循环，用于将data指向的字符串中的数字提取并存入value，第9-12行则是对不满足9位的数填0，最后通过13和14行输出结果。我们通过源代码很容易理解其逻辑，但是将其编译为二进制代码（并通过Ghidra反编译得到伪C代码）后，由于编译优化（-O2）的影响，它的部分逻辑表达形式发生了变化（虽然功能仍然等价）。源代码中体现“解析至多9位字符”的条件语句在反编译变体中却以`if(8 < uVar3)`的形式表达。这在一定程度上增加了语义理解的困难，甚至会误导方法的预测。例如使用SymGen、XFL和LLM预测时的这一种可能结果如表xxx所示



% ### Motivation

% 近年来，LLM已经在许多源码级任务上展现出非凡的有效性，以SymGen、FoC为代表的研究也让我们看到了LLM在理解二进制代码语义上的潜力，因此我们相信LLM在理解二进制代码语义上仍然是一种有效的策略。为了解决C1的问题，我们不对小参数模型（如CodeLlama）进行微调，而是选择使用较大参数的通用LLM作为基础组件，并将其搭建成一个Multi-Agent系统。在这种策略下我们不再需要精心微调LLM，这在一定程度上实现了更强的泛用性效果（微调大参数模型及其消耗资源与时间）。

% 为了解决C2，HyBinMAS会首先处理目标so文件，并通过动静态混合分析的方式确定函数调用关系，并在预测函数名时显示考虑这样的调用关系。我们观察到逆向工程师通常采用递归的思想处理函数调用：即如果FUN_A调用了FUN_B，那么他们会先分析FUN_B，并将FUN_B的语义理解结果提供给FUN_A。受到这一事实的启发，HyBinMAS在恢复函数名时也会进行相似的过程：先对目标函数的函数中调用的函数进行名称恢复，再利用得到的信息完成对目标函数的名称恢复。以图xxx为例，通过分析FUN_00125a60的调用关系，我们可以确定param_2实际指向的函数，并分析出它的逻辑与copy相关，据此推测出了如图xxx所示的更准确的答案。如果没有这样的策略，我们只能知道该函数在进行一个递归遍历操作，而不知道它在遍历执行什么具体的操作。

% 我们观察到一名资深的逆向工程师不只会“看”代码，还会“跑”代码。他们通过“看”代码初步理解其语义信息后，会尝试构造一些测试样例，并执行目标函数，观察其动态行为是否与自己之前的推测一致，据此进行下一步的理解，我们认为这正是解决C3的一种有效方法。以ParseNanos函数为例，我们看到表xxx中LLM的预测结果基本吻合，但是在关键的位数中出现了错误，它正是被编译优化的结果误导了。根据其语义信息，为期提供表xxx所示的测试样例并执行，我们发现测试样例4期望只提取前8个字符，但是实际上9个字符均完整地得到了。这样的执行信息会被总结成了反馈信息指导LLM修正前一轮的推测结果。这一过程将迭代多轮，最终预测出了表xxx HyBinMAS列所示的更准确的语义信息。

% 另外，我们强调并不是所有函数都能够被正确地执行。图xxx是一个更为复杂的例子（反编译代码见附录）。LLM初次语义理解的结果为`remove`，这样的结果显然不够准确。但因为它有一个自定义结构体指针的参数，因此在生成测试样例并执行时很容易出现非法访问而导致程序异常退出的情况。通过利用额外的LLM将测试样例和执行失败的日志信息总结成反馈信息并提供给LLM，HyBinMAS实现了更为准确的推测结果`remove_env`。



\section{Method Design}
\subsection{Overview}
Inspired by the process that human experts employ to manually comprehend the semantics of stripped functions and recover their names, we introduce \textbf{HyBinMAS}, a novel framework for binary function name recovery founded on hybrid analysis and a Multi-Agent system. HyBinMAS identifies the critical information integral to the function name recovery workflow and distills it into two fundamental services: the \textbf{HyBrid Static-and-Dynamic Analyzer (HSDA)} and the \textbf{Executor}. To fully leverage this information, HyBinMAS conceptualizes the recovery process into three crucial phases, each implemented by a dedicated Agent: the \textbf{Semantic Restorer (SR)}, the \textbf{Test Generator (TG)}, and the \textbf{Execution Validator (EV)}. To maximize the synergistic potential of these Agents and enhance the accuracy of the final name prediction, we have meticulously orchestrated their collaboration within the workflow depicted in Figure \ref{fig:overview}. Furthermore, Algorithm \ref{alg:HyBinMAS} precisely delineates the interplay between these Agents and the core services.
\begin{figure}[h] % [h] 表示图片放置在当前位置
    \centering
    \includegraphics[width=0.5\textwidth]{image/Overview.drawio.png} % 调整图片宽度为文本宽度的50%
    \caption{An overview of the Multi-Agent system based on Frida and Ghidra} % 添加图片标题
    \label{fig:overview} % 设置图片标签，方便引用
\end{figure}

The process is initiated by providing HyBinMAS with its inputs: the binary file, control parameters outlined in Algorithm \ref{alg:HyBinMAS}, and a target function uniquely identified by its offset. The process then unfolds as follows: \textbf{(\textit{i})} The \textbf{HSDA} performs a hybrid static-dynamic analysis on the target function and its containing binary file, yielding the function's assembly code, decompiled pseudo-code, call graph, and other pertinent metadata (Line 1 of Algorithm \ref{alg:HyBinMAS}). \textbf{(\textit{ii})} The \textbf{SR} leverages this comprehensive information to generate an initial predicted function name and a semantic summary (Line 2 of Algorithm \ref{alg:HyBinMAS}). \textbf{(\textit{iii})} Subsequently, the \textbf{TG} synthesizes a suite of suitable test cases based on the decompiled code from the HSDA and the name and summary provided by the SR (Line 4 of Algorithm \ref{alg:HyBinMAS}). \textbf{(\textit{iv})} The \textbf{Executor} then proceeds to run each test case generated by the TG, capturing both the execution traces and any error information. \textbf{(\textit{v})} Finally, the \textbf{EV} analyzes the outcome of each execution. Depending on whether a test case succeeded or failed, it selects an appropriate analyzer to process the results, summarize the collective execution feedback from the entire test suite and replay it to the SR for refinement and improvement.

\begin{algorithm}
  \caption{HyBinMAS}
  \label{alg:HyBinMAS}
  \begin{algorithmic}[1]
    \REQUIRE $o$: The Offset of target function, $B$: The binary containing target function, $M$: The maximum iterations, $R$: The maximum retries for execution failure, $K$: The maximum numbers of test cases
    \ENSURE $l$: The predicted function name

    \STATE $\langle A, P, CG, metadata \rangle \gets \text{HSDA}(o, L)$
    \STATE $\langle n, s \rangle \gets \text{SR}(CG, o, A, P, M, \text{NULL})$
    \FOR{$m \gets 1$ \textbf{to} $M$}
        \STATE $T \gets \text{TG}(n, s, P, K)$
        \STATE $F \gets []$
        \FORALL{$t \in T$}
            \STATE $\langle err, args, ret, trace \rangle \gets \text{Executor}(t, R)$
            \IF{$err \ne \text{NULL}$}
                \STATE $f \gets \text{FallbackAnalyer}(t, err)$
            \ELSE
                \STATE $f \gets \text{NormalAnalyzer}(t, args, ret, trace)$
            \ENDIF
            \STATE $F.\text{push}(f)$
        \ENDFOR
        \STATE $feedback \gets \text{generate\_feedback}(F)$
        \IF{$\text{consistent\_check}(feedback)$}
            \RETURN $l$
        \ENDIF
        \STATE $\langle n, s \rangle \gets \text{SR}(CG, o, A, P, metadata, feedback)$
    \ENDFOR
    \RETURN $l$
  \end{algorithmic}
\end{algorithm}

As illustrated by Algorithm \ref{alg:HyBinMAS}, HyBinMAS performs at least one initial static analysis pass with the SR. This is followed by an iterative refinement loop (Lines 6-19), which repeats steps \textbf{(\textit{iii})}, \textbf{(\textit{iv})}, \textbf{(\textit{v})} and \textbf{(\textit{ii})} for up to $M$ iterations to progressively improve the initial semantic inference of SR. In the following sections, we will elaborate on the details of each step.

% 受到人工理解Stripped functions语义信息并恢复其函数名的一般流程的启发，我们提出了一个基于混合分析和Multi-Agent的二进制函数名称恢复框架——HyBinMAS。HyBinMAS洞察了函数名称恢复流程中用到的重要信息并将其抽象为两个基本服务：HyBrid Static-and-JDynamic Analyzer (HSDA)和Executor。为了充分利用这些信息，HyBinMAS建模了三个重要环节，并设计了对应的Agent实现他们：Semantic Restorer（SR）、Test Generator（TG）和Execution Validator（EV）。为了能够最大程度地发挥不同Agent的能力以提升最终函数名预测的小小果，我们按照图xxx所示的工作流精心将它们组织在一起。算法xxx则准确地描述了不同Agent和服务之间的交互关系。

% 对于一个待分析的函数，可以使用它相对于二进制文件的起始偏移地址唯一标识，HyBinMAS将该函数和所在的二进制文件作为输入（以及算法xxx所示的控制算法流程的必要参数）。（1）HSDA对目标二进制函数和so文件进行动静态混合分析，并得到目标函数的反汇编代码、反编译代码、调用图和其他元信息（算法xxx中的第1行）。（2）SR充分利用这些信息，生成预测的函数名和摘要（算法xxx中的第2行）。（3）TG根据HSDA输出的反编译代码和SR输出的函数名和摘要生成合适的测试样例集（算法xxx第4行）。（4）Executor逐个执行TG生成的测试样例，并获取执行信息和错误信息。（5）EV逐个分析Executor的执行结果，根据是否执行成功选择合适的分析器进行分析，总结测试样例集的执行信息并反馈给SR进行修正与改进。如算法xxx所示，HyBinMAS至少执行一次SR分析静态信息，并在第6-19行通过迭代至多M次步骤（3）（4）（5）（2）来修正第一次SR输出的语义信息。接下来我们会讨论每一步骤的细节。


% Inspired by the typical workflow of human understanding of Android Native Functions, we have developed BinMAS, a Multi-Agent system based on tools such as Ghidra and Frida. BinMAS decomposes this workflow into five sub-tasks, each modeled and handled by a distinct Agent: the Static Information Agent (SIA), the Execution Agent (EA), the Semantic Analysis Agent (SAA), the Test Case Generation Agent (TCGA), and the Validation Agent (VA). To maximize the capabilities of each Agent and enhance the overall performance, we organize them according to the structure shown in \ref{fig:overview}. Herein, hollow and dashed arrows represent the data flow between different Agents, while solid arrows indicate data flow between components within the same Agent. Additionally, the components and their contents for each Agent are denoted by a consistent color scheme (e.g., EA is marked in green).

% For a target function, identified by its offset relative to the base address of the .so file, SIA is first employed to acquire its static information. This static information is then fed into the SAA as its primary input, prompting the corresponding LLM to generate semantic information about the function, including a proposed function name and summary. This semantic information, along with the static information from SIA, serves as input for the TCGA to construct corresponding test cases. Within the EA, we have established a Frida-based runtime environment for information acquisition. It accepts the test cases from TCGA and the basic information from SIA to execute the target function and capture its dynamic execution information (see Section xxx for details). The outputs from the preceding four Agents are all provided as part of the prompt to the VA. The VA's role is to validate the correctness of the semantic understanding produced by the SAA. If the understanding is deemed correct, it is accepted as the final result. Otherwise, the VA must provide detailed explanations and suggestions for revision. This feedback, along with the test cases and dynamic execution information, is passed back to the SAA for refinement. This process iterates until the VA's validation is successful or a predefined threshold is reached. In the following sections, we will discuss each agent in detail.


\subsection{Hybrid Static-and-Dynamic Analyzer Design}
\label{sec:hsda_design}

As the inaugural component of the \textbf{HyBinMAS} framework, the Hybrid Static-and-Dynamic Analyzer (HSDA) accepts a target function (e.g., \texttt{libnative-lib.so[+0x1234]}) and its containing binary as initial inputs to perform a hybrid static-dynamic analysis. Given that the HSDA serves as a foundational service, its output provides the essential basis for all subsequent stages. Existing research, such as SymGen, has demonstrated that decompiled code contributes more significantly to semantic comprehension than raw assembly code. Therefore, we designate the decompiled pseudo-code as a primary output of the HSDA module for downstream use. Building upon this foundation, the HSDA incorporates the following key advancements:

We recognize that such pseudo-C code does not strictly represent the actual execution order. In contrast, assembly code maintains a high degree of fidelity with the sequence of executed instructions. Consequently, we designed the HSDA to also output the assembly code, which is specifically utilized by the downstream Execution Validator (EV). Furthermore, the HSDA is leveraged to extract the function's signature, encompassing the types and storage locations (e.g., registers or stack offsets) of its parameters and return value. This signature information not only provides fundamental support for the Semantic Restorer's (SR) inference process but also serves as a critical prerequisite for generating and executing test cases.

To address Challenge C2, we posit that a comprehensive understanding of function call context is indispensable for semantic analysis. The prevalence of indirect calls in C/C++ (as exemplified in Figure~\ref{fig:indirect_call_example}) renders static analysis alone insufficient for constructing a complete call graph, which in turn impairs the efficacy of name recovery. We therefore begin by defining a formal structure, the \textbf{Attributed Call Graph (ACG)}, denoted as \( \mathcal{G} = (\mathcal{V}, \mathcal{E}) \).

The vertex set, \(\mathcal{V} \subseteq \mathcal{N} \times \mathcal{L} \times \mathcal{S}\), consists of tuples \(v = (n, l, s)\), where \(\mathcal{N}\) is the set of unique function identifiers, \(\mathcal{L}\) is the space of potential function names, and \(\mathcal{S}\) is the space of semantic summaries. The edge set, \(\mathcal{E} = \mathcal{E}_{s} \cup \mathcal{E}_{d}\), represents the union of static and dynamic calls. An edge \(e \in \mathcal{E}\) is a tuple \((n_u, n_v, cs, \text{exp}_{\text{call}})\), where \(n_u\) is the caller, \(n_v\) is the callee, \(cs\) is the call site address, and \(\text{exp}_{\text{call}}\) is the corresponding call expression in the decompiled code.

The process begins with an initial ACG derived from static analysis tools. This initial graph is partially attributed: it contains vertices that are not yet populated with semantic information (names and summaries) and a set of fully attributed static edges, \(\mathcal{E}_s\). The enrichment of vertex attributes is a core responsibility of the SR component, which will be detailed later. To populate the dynamic edge set \(\mathcal{E}_d\), we first capture dynamic call traces as a set of pairs \((cs, ct)\), where \(cs \in \mathcal{CS}\) is a call site and \(ct \in \mathcal{N}\) is its dynamically resolved target. We then formalize the augmentation process with the following definition for \(\mathcal{E}_d\):
\begin{equation}
\label{eq:dynamic_augment}
\mathcal{E}_d = \{ (n_u, ct, cs, \text{exp}_{\text{call}}) \mid \exists n_u \in \mathcal{N} \text{ where } cs \in \text{Sites}(n_u) \land (cs, ct) \text{ is an observed dynamic call} \}
\end{equation}
In Equation~\ref{eq:dynamic_augment}, \(\text{Sites}(n_u)\) denotes the set of all call sites within function \(n_u\), and \(\text{exp}_{\text{call}}\) is the decompiled expression aligned with the call site \(cs\).

This augmentation process is visually depicted in Figure~\ref{fig:graph_augmentation}. Initially, the ACG contains only the static call relationships (e.g., between functions A, B, and D). By applying the rule of Equation~\ref{eq:dynamic_augment}, we enrich the graph with the newly discovered dynamic call edges, represented by the dashed blue arrows.


% 作为HyBinMAS的开端，HSDA接受一个待分析的函数（例如例如libnative-lib.so[+0x1234]）及其所在的二进制作为初始输入，进行动静态混合分析。由于HSDA是HyBinMAS框架的基础服务之一，其输出信息是其余环节的重要依据。现有的研究（SymGen）表明，相较于反汇编代码，反编译代码对函数语义理解的贡献度更大，所以我们也将反编译代码作为HSDA模块的一个重要输出供后续使用。在此基础上，HyBinMAS还做了如下突破：

% 我们洞察到这样的伪C代码不能严格代表实际的执行顺序。相反，反汇编代码可以与实际执行的指令具有高度地对齐性。偶一我们在设计HSDA时也将反汇编代码作为另一个输出成分以供后续EV使用。我们还利用HSDA获取了函数的签名信息，包括参数与返回值的类型、存储位置等。这些信息除了为后续SR推理函数名提供基本的支持，还将作为生成与执行测试样例的重要依据。

% 为了解决C2，我们指出在进行语义理解时应当考虑函数调用情况，而且由于C/C++中广泛存在间接调用的使用（例如图xxx），仅考虑静态调用信息将难以保证调用链的完整性，进而影响函数名恢复的效果。所以我们首先定义了带属性的调用图ACG = <AN, AE>，其中节点集AN = N * L * S的每个元素是一个<n, l, s>三元组，分别表示当前节点的标识符、SR预测的函数名和摘要。而边集AE= AE_d + AE_s = N * N * CS * EXP代表了函数之间存在的静态调用和静态调用关系（均由调用者指向被调用者），由<n_u, n_v, cs, exp_{call}>表示这条边所对应的调用点以及该调用点在反编译代码中对应的调用表达式。

% 其次使用静态分析工具可以获得初始的ACG，它由不携带属性的节点信息和包含属性的静态调用边信息组成，节点的属性信息的补充将会在SR部分详细交待。而对于动态调用边信息，我们首先获得<cs, ct> \in CS * N的调用信息，并定义了公式xxx完成动态增强的过程：
$AE_d = \{ \langle n_u, ct, cs, exp_{call} \rangle | \forall n_u \text{ contianing cs}, ct, cs, exp_{call} \text{ is calling expression aligned with cs} \}$
% 图xxx的蓝色箭头也描述了动态增强的过程，在没有使用动态增强之前，ACG中只有A、B、D之间存在静态调用关系，利用公式xxx，我们在图中用虚线箭头补充了动态调用的关系。


\begin{algorithm}
  \caption{Dynamic Enhanced Call Analysis}
  \label{alg:CallAnalysis}
  \begin{algorithmic}[1]
    \REQUIRE $S$
    \ENSURE $CG$: The dynamic enhanced Call Graph

    % 对于动态调用分析得到的每个调用对<callsite, calltarget>
    \FORALL{$(cs, ct) \in \text{dynamic\_call\_analysis}(binaries)$}
        \STATE {$caller \gets \text{get\_function}(cs)$}
        \STATE {$S_{caller} \gets S.pull(caller)$}
        \STATE {$S_{caller} \gets \text{add\_call\_relation}(S_{caller}, \langle cs, ct \rangle)$}
        \STATE $S.push(S_{caller})$
    \ENDFOR

    \STATE {$ACR \gets \{\}$}
    % 执行对齐
    \FORALL{$\langle cs, exp_{call} \rangle \in \text{extract\_expression}(binaries)$}
        \STATE {$cts \gets \text{get\_ct}(S_{caller}, cs)$}
        \STATE {$\langle exp_{call}, cts \rangle \in ACR$}
    \ENDFOR
    \STATE {$CG \gets \text{construct\_callgraph}(ACR)$}
    \RETURN $CG$
  \end{algorithmic}
\end{algorithm}





\subsection{Executor Design}

The Execution Agent (EA) represents a key innovation within the BinMAS framework. To the best of our knowledge, this work is the first in the domain of binary code semantic understanding to incorporate actual runtime behavior. Existing research on binary semantic understanding predominantly considers only static semantic information, lacking the capability to incorporate dynamic execution information (although SymLM considers execution behavior, it simulates dynamic actions using static call graph information, which is still fundamentally static in nature). This limitation arises because their target, stripped binaries, is overly broad, spanning multiple architectures and diverse application types, which makes it challenging to construct a unified execution environment. In contrast, BinMAS circumvents this issue by focusing on Android Native libraries, leveraging the inherent executability of apps to naturally construct a runtime environment.

Specifically, the EA leverages the powerful dynamic instrumentation tool, Frida, to intercept target functions. Frida can locate functions in two ways: by name or by offset address. Since many symbols are stripped in release builds of applications, we must rely on offset addresses to locate the target functions. This process is known as Frida hooking. Following the Frida hooking operation, we employ Stalker, a feature provided by Frida for instruction tracing, to capture the runtime behavior of the target function. We focus on two types of information: the function's runtime parameters, return value, and its execution trace.

\subsubsection{Acquiring Trace Information}

We construct the trace using CPU context information, specifically the state of the registers. This approach is chosen because the ARM64 architecture provides a significantly larger number of programmer-visible registers compared to the x86 architecture. As shown in Table~\ref{tab:arm_registers}, the ARM64 architecture offers 32 general-purpose registers and 32 vector registers. The general-purpose registers can act as units of different sizes (ie. 1B, 2B, 4B and 8B) depending on the operation type. The vector registers are even more versatile, supporting parallel computations in addition to using their lower 32 bits to represent \texttt{float} values and their lower 64 bits for \texttt{double} values. Furthermore, these registers are preferentially used for passing arguments and return values, as well as for storing temporary variables.

Since the execution of each instruction alters the context state, providing context at an instruction-level granularity is a trivial approach. However, given the large number of registers in the ARM64 architecture, providing the complete register state for every instruction would significantly exceed the context window of an LLM and could also induce hallucinations. Therefore, we implemented the following optimizations:
\begin{enumerate}
    \item \textbf{Minimize Register Information:} We select all general-purpose registers. For vector registers, we only consider common access patterns, namely as \texttt{float} and \texttt{double} values.
    \item \textbf{Differential Tracing:} We provide the complete register state only at the function's entry and exit points. For each intermediate instruction, we only report the registers that have changed relative to the previous step. Subsequent experiments demonstrate that this approach helps the model focus on the semantics of individual instructions, thereby improving the overall accuracy of semantic understanding.
\end{enumerate}

\subsubsection{Acquiring Parameters and Return Values}

Although the standard calling convention in the ARM64 architecture dictates that parameters and return values are passed via registers, which implies that, in most cases, this information would already be captured within the execution trace, we find it beneficial to handle them explicitly. Because parameter and return value information is highly indicative of a function's semantics, highlighting it can significantly aid the LLM in its semantic comprehension. To this end, we leverage Ghidra's static analysis capabilities to precisely identify these values. This process also enables the discovery of parameters stored on the stack.


\subsection{Semantic Restorer (SR) Design}
\label{sec:sr_design}

The Semantic Restorer (SR) constitutes the core component of the \textbf{HyBinMAS} framework. Its primary responsibility is to generate the predicted function name, which ultimately serves as the final output of the entire system. To address Challenge C2 and enhance the efficacy of name recovery, the SR's analysis of any given function must incorporate information about its callees. To facilitate the propagation of semantic information between functions, we designed the SR to output not only a predicted name but also a functional summary. This summary is widely recognized in related binary analysis research as an effective means of characterizing function semantics.

In the workflow of traditional binary function name recovery, the aggregation and interpretation of available information have historically been manual endeavors. This manual process is a primary contributor to the tedious and challenging nature of reverse engineering, as it demands significant domain expertise and careful consideration of how to properly leverage both static and dynamic information. Previous research has already demonstrated the non-negligible potential of Large Language Models (LLMs) in both source-level and binary-level tasks. Consequently, we positioned an LLM as the central component of \textbf{HyBinMAS} from its inception, with the SR being a prime instantiation. Nevertheless, a significant gap persists between the outcomes of existing methods (e.g., SymGen, XFL) and the desired results. Upon careful scrutiny of their approaches, we identified a common shortcoming: they do not effectively leverage contextual information, often relying solely on the target function's decompiled code as the basis for inference. Building upon the insights from existing research while addressing their limitations, we have engineered the SR to accept a richer set of inputs:

\begin{enumerate}
    \item \textbf{Static Analysis Artifacts:} As indicated by the black arrow from HSDA to SR in Figure~\ref{fig:xxx}, this includes the decompiled code, call graph, and metadata about the binary and the function under analysis. This information forms the foundation for semantic understanding, where the inclusion of the call graph is specifically intended to improve inferential accuracy.
    \item \textbf{Generated Test Cases:} Represented by the dashed orange arrow from TG to SR, this provides the SR with the specific inputs that will be used to probe the function's dynamic behavior.
    \item \textbf{Execution Feedback:} The red arrow from EV to SR signifies the feedback information consolidated by the EV. This feedback is derived from the execution traces (or errors) produced by the Executor using the TG's test cases (see Section~\ref{sec:ev_details} for details).
    \item \textbf{Callee Semantics:} The semantic information (i.e., the function name and summary) of the functions called by the target function.
\end{enumerate}

We meticulously orchestrate this multifaceted information into a structured prompt, as depicted in Figure~\ref{fig:xxx}, to serve as the input for the LLM. It is important to note that during its initial inference pass, the SR can only utilize the static information from item (1). However, in subsequent iterations, \textbf{HyBinMAS} enriches the SR's context with dynamic information derived from the Executor, TG, and EV, enabling the LLM to progressively refine its predictions.

Building upon the Attributed Call Graph (ACG) obtained from the HSDA, we abstract the core operation of the Semantic Restorer (SR) as a state transition function. This function describes how applying the SR to a specific node \(n\) in the current graph \(\mathcal{G}\) results in a new graph state, \(\mathcal{G}'\):
\begin{equation}
\label{eq:sr_transition}
\mathcal{G}' = \text{SR}(n, \mathcal{G})
\end{equation}
The effect of this transformation is to produce an updated graph, \(\mathcal{G}'\), where the attributes of the vertex \(n\) have been newly populated. The difference between the graphs can be expressed as the augmentation of \(n\) with a predicted name \(l\) and summary \(s\). This process, where the SR enriches a node with semantic information, is visually depicted by the orange arrow in Figure~\ref{fig:xxx}.

A critical prerequisite for applying this transition function is to process the graph's nodes in a bottom-up, reverse topological order. To facilitate this, we first operate on the **transpose** of the ACG. This operation, as illustrated by the green arrows in Figure~\ref{fig:xxx}, involves reversing the direction of every edge in the graph. The resulting structure, denoted as \(\mathcal{G}^T\), ensures that when we traverse the nodes, a function's callees are typically analyzed before the function itself, allowing semantic information to propagate upwards through the call hierarchy.

We have also designed the workflow algorithm for the SR as detailed in Algorithm~\ref{alg:sr_workflow}. We recognize that, excluding recursion, a call graph is a Directed Acyclic Graph (DAG), from which a reverse topological order can be derived. In cases involving recursion, which manifest as back edges in the graph, we can temporarily remove these edges to once again obtain a DAG. The \texttt{topo\_sort} function on Line 2 of Algorithm~\ref{alg:sr_workflow} is responsible for this task. Finally, Lines 5-11 of the algorithm describe the process of enhancing the LLM's reasoning process with the collected dynamic information.

\begin{algorithm}
  \caption{Workflow Algorithm for SR}
  \label{alg:sr_workflow}
  \begin{algorithmic}[1]
    \REQUIRE $ACG$: The Attributed Call Graph from HSDA, $o$: The offset of target function, $cases$ The test cases from TG, $feedback$: The feedback from EV
    \ENSURE $l$: The predicted function name

    \STATE {$DACG \gets \texttt{GetDominatedACG}(ACG, o)$}
    \STATE {$W \gets \texttt{ReverseTopoSort}(DACG)$}

    \STATE {$NC \gets \varnothing$}
    \WHILE{$W \ne \varnothing $}
        \STATE {$w \gets \texttt{PopFirst}(W)$}
        \IF{$w \in NC$}
            \STATE {$N \gets \texttt{Get}(w)$}
        \ELSE
            \STATE {$N \gets \texttt{QueryLLM}(w)$}
            \STATE {$\texttt{Set}(w, n)$}
        \ENDIF
    \ENDWHILE

    \IF{$\texttt{UseDynamicInfo}()$}
        \STATE {$N \gets \texttt{QueryLLM}(w, cases, feedback)$}
    \ENDIF

    \RETURN $l$
  \end{algorithmic}
\end{algorithm}

% SR是HyBinMAS的核心组件，它的主要输出信息之一是预测出的函数名，这同样也是HyBinMAS的最终输出。考虑到C2的存在，为了提升函数名恢复的效果，SR在分析一个函数时还需要考虑它所调用的函数。为了能够在不同函数之间传递语义信息，我们在设计SR时除了用于预测函数名之外，还需要输出函数的摘要信息，这在许多二进制逆向相关的研究中都被认为是表征函数语义的有效方式。

% 在传统二进制函数名恢复任务的流程中，如何将现有的信息汇总起来并将进行推理一直都由人工负责，这也是二进制逆向任务繁琐困难的原因之一，因为它同样需要一定的领域知识，而且需要考虑如何妥善地利用这些静态信息与动态信息。先前的研究已经展现了LLM在源码级任务、二进制级任务中具有不可忽视的潜力，因此我们在提出HyBinMAS时就将LLM作为核心组件，SR就是其中之一。但是现有的研究的成果（SymGen、XFL）与理想结果仍然存在较大的鸿沟，我们仔细学习与审视了他们的方法，发现它们并没有很好地利用上下文信息，而通常仅仅将目标函数的反编译代码作为推断的依据。在借鉴了现有研究的经验、总结了他们的不足的基础上，我们为SR设计了如下的输入信息：

% 1. 图xxx中由HSDA到SR的黑色箭头表示了反编译代码、调用图、关于待分析文件和函数的元信息等。这些信息是语义理解的基础，我们通过引入调用图来增加函数语义理解的准确度。
% 2. 从TG到SR的橙色虚线箭头代表了生成的测试样例
% 3. 从EV到SR的红色箭头则是EV根据TG的测试样例和Executor的执行信息总结得到的反馈信息（具体见xxx）
% 4. 目标函数所调用的函数的语义信息（函数名和摘要）

% 我们将这些信息按照图xxx所示的方式妥善编排成提示词作为LLM的输入。值得注意的是，SR在首次推测函数名时，它能够利用的信息只有（1）中的静态信息。但在后续的迭代中，HyBinMAS就可以根据Executor、TG和EV的输出，为SR提供必要的动态信息以改进LLM的推测结果。

% 为了妥善处理函数调用的情况，我们在SR中设计了算法xxx所示的工作流算法：

% 我们在HSDA获得的ACG的基础上，将SR抽象为公式xxx所示的状态转移方程，它表示对于当前状态的ACG，且指针位于节点n处，经过SR之后ACG的状态变为ACG^\prime，其中ACG^\prime - ACG = <n, l, s>，也即SR对节点n所代表的函数名和摘要信息进行了补充。图xxx的橙色箭头形象地描述了这一过程。值得注意的是，在应用转移方程SR之前还将ACG中所有的边按照绿色箭头所示进行了逆向操作
$$ACG^\prime = SR(n,ACG)$$

% 我们知道，在不考虑递归的情况下，调用图是一个有向无环图，此时我们可以获得它的反向拓扑序列，而对于递归调用，它必然存在一条回边，我们可以将这样的回边删除，仍然可以得到一个有向无环图，算法xxx中的第2行的topo_sort正是负责这一任务。算法5-11行则描述了使用动态信息增强LLM推理的过程。




\subsection{Test Generator Design}
\label{sec:tg_design}

We reiterate that semantic understanding based solely on static information is often insufficient and imprecise. To emulate the process of manual dynamic debugging, it is necessary not only to provide execution support via the Executor but also to construct appropriate test cases that can "trigger" the target function's execution. Within \textbf{HyBinMAS}, this functionality is realized by the Test Generator (TG). However, test case generation is a non-trivial task, particularly because \textbf{HyBinMAS} requires test cases that can intelligently validate the function's anticipated logic. Consequently, we employ a Large Language Model (LLM) as the core of the TG. Nevertheless, the design of the TG should navigate these significant obstacles:

\begin{enumerate}
    \item \textbf{Unlike conventional test case generation, the TG must infer potential parameters and their corresponding types---including primitive types, their derivatives (e.g., pointers), and custom structures---directly from decompiled code, without access to the original source. This significantly complicates the generation process.}
    \item \textbf{The generated test cases must be concretely applicable to the target function, meaning they must be translated into actually executable code.}
    \item \textbf{Accurately recovering the full parameter and return value information from stripped binaries is, in the general case, an undecidable problem.}
\end{enumerate}

To manage these complexities, we decompose the TG into two core components as depicted in Figure~\ref{fig:xxx}: the \textbf{Orchestrator} and the \textbf{Materializer}, responsible for the high-level design and the concrete implementation of the test cases, respectively.

First, the \textbf{Orchestrator} is tasked with designing the test cases.  It receives the target function's semantic information and generates a test plan articulated in natural language, aimed at verifying the logic implied by the SR's semantic description, namely the function summary. The rationale for this separation is twofold: different architectures and instrumentation tools often demand distinct implementation strategies and, thus, different domain knowledge. Entrusting both the design and implementation to a single LLM would dramatically increase the risk of context overflow. To facilitate implementation by the Materializer, the Orchestrator's design must explicitly specify the input values for parameters as well as the expected output values for both parameters and the function's return value. We therefore structure the Orchestrator's output according to the format shown in Figure~\ref{fig:xxx}. Obtaining this information is not straightforward, especially for pointer-type parameters. Therefore, our Orchestrator implementation first uses an LLM for test case generation. To validate  correctness of pointer operations, we then leverage program slicing to extract parameter-related statements from the decompiled code, which a secondary LLM reviews to ensure the integrity of these critical operations within the proposed test plan.

Second, the \textbf{Materializer} component translates the natural language descriptions from the Orchestrator into executable code using specific memory manipulation primitives; this process is effectively a code generation task. To address obstacles (1) and (2), we must furnish the LLM with sufficient domain-specific knowledge. In our reference implementation, for example, we provide the LLM with the relevant memory manipulation APIs from the Frida toolkit (see Appendix for details). Moreover, instead of presupposing the form of the function's arguments, we prompt the LLM to generate a dedicated function (e.g., \texttt{writeArgs}) to marshal the parameters according to the test case's input requirements. For the same reason, the LLM is also instructed to generate a corresponding \texttt{readArgs}-like function to read the output parameters and return them as a string containing explanatory details. The return value is handled similarly. Figure~\ref{fig:xxx} illustrates the materialized implementation corresponding to the design from Figure~\ref{fig:xxx}.

Finally, acknowledging the difficulty posed by challenge (3), we anticipate that a test case initially generated may fail to execute. We therefore establish an iterative improvement between the Executor and the TG: When the Executor fails to run a materialized test case, it reports back essential information via error messages or exceptions, which is then used to refine the implementation in the Materializer. Successfully executed test cases, along with their execution traces, are passed to the EV for analysis. For the same reason, we have incorporated a dynamic execution fallback strategy: if the Materializer's implementation attempts fail more times than a specified threshold, \textbf{HyBinMAS} will abort the execution of that particular test case and pass the error information directly to the EV for analysis.

% 我们再次强调，仅仅根据静态信息来进行语义理解时不准确的，为了模拟人工动态调试的过程，我们除了在Executor部分提供相关的运行支持之外，还需要为其构造合适的测试样例并“触发”目标函数的执行。在HyBinMAS中，我们设计了TG来实现这样的功能。然而，测试样例生成同样不是一个trivial的任务，尤其是HyBinMAS还要求测试样例需要智能地验证目标函数的预期逻辑。所以我们使用LLM作为TG的核心。然而，设计TG时，需要解决如下关键问题：（1）不同于一般的测试样例生成任务，TG需要在不知道源码的情况下分析反编译代码中可能用到的参数及其对应的类型，包括基本类型及其衍生类型、自定义类型或其他更复杂的情况，这无疑增加了测试样例生成的困难性。（2）生成的测试样例必须能够被正确地应用到目标函数上，也就是说它是一段实际可执行的代码。（3）准确恢复stripped binaries中的函数参数与返回值信息是一个不可判定任务。所以我们将TG拆分为图xxx所示的两个核心组件：Orchestrator和Materializer，分别负责测试样例的设计编排与实现

% 首先，我们使用Orchestrator完成测试样例的设计任务，它接收目标函数的语义信息测试目标，生成一种由自然语言编写的测试样例i验证该语义信息所体现的逻辑。这样设计的合理性在于：对于不同的架构和不同工具，我们通常需要不同的实现方式（例如编写不同的测试代码），这意味着不同的领域知识。如果直接将测试样例的设计与实现都交给同一个LLM完成，将极大增加上下文溢出的风险。为了便于Materializer实现测试样例，我们需要在设计时明确地输出参数的输入值、参数与返回值的预期值。所以我们按照了图xxx所示的格式作为Orchestrator的输出。请注意，要想正确地获取这些信息并不是轻而易举的事，尤其是当参数是指针类型时，所以我们在实现Orchestrator时，除了使用一个LLM生成测试样例，我们还利用程序切片技术将反编译代码中与参数相关的语句提取出来，并利用另一个LLM对其中涉及的指针操作进行review，确保测试样例中指针操作的正确性。

% 其次，Materializer组件会根据Orchestrator的自然语言描述，使用特定的内存操作代码实现出来，所以这实际上是一个函数生成任务。为了应对问题（1）与问题（2），我们需要给LLM足够多的领域知识。以本文的实现为例，我们需要提供Frida中内存操作的相关API（详细件附录）。而且我们不能提前假设参数的形式，而是应当提示LLM生成一个函数（例如writeArgs）来修改参数使其满足测试样例的输入要求。另外，处于同样的原因，我们也需要让LLM生成一个形如readArgs的函数来读取参数并以一个包含解释信息的字符串的形式返回（返回值也有相同的要求）。图xxx展示了图xxx所对应的实现结果。

% 最后，我们承认问题（3）的苦难行，单次生成的测试样例很有可能在Executor上执行不起来，所以我们增加了Executor和TG之间的交互：当Executor无法执行TG实现的测试样例时，它会以报错或异常的形式反馈必要的信息并用于Materializer的改进。最终通过的测试样例会与Executor执行成功的信息一同交给EV分析。处于同样的原因，我们还涉及了动态执行的fallback策略：当Materializer实现失败次数超过了指定阈值时，HyBinMAS会放弃执行该测试样例，并直接将错误信息和对应的测试样例交给EV进行分析。




\subsection{Validation Agent Design}

The rationale for designing the Validation Agent (VA) stems from a fundamental limitation of dynamic analysis: a single execution trace, while concrete, does not guarantee full path coverage. This means that a single run is unlikely to exercise all execution paths, potentially omitting critical logic. To address this, BinMAS employs the VA to validate the semantic understanding produced by the SAA. If the validation is successful, it is concluded that the SAA's output accurately reflects the logic of the target function. Conversely, if validation fails, the SAA's output is considered discrepant. The VA then provides a detailed explanation and suggestions for revision. This feedback, along with the test case and execution trace, is passed back to the SAA to guide the next iteration of analysis.

Evidently, the outcome of the VA's validation critically influences the number of iterations in the workflow and, in turn, affects the overall efficacy of our method. To minimize the potential for error in the VA's judgment, we furnish it with a rich set of contextual information. As depicted in Figure~xxx, the function metadata from SIA is included as part of the VA's prompt template. The test case generated by the TCGA is another essential component, and the execution trace obtained by the EA is also supplied as input. The output from the SAA serves as the object of validation. Furthermore, we incorporate specific instructions in the prompt to guide the model to produce output in a structured format (e.g., a pass/fail decision, the rationale for failure, etc.).



\section{Implementation and Evaluation}

\subsection{Implementation and Setup}

Our implementation leverages \textbf{Ghidra v11.3.2} to procure static information, including disassembly, decompiled pseudo-code, the static call graph, and function signatures. For the acquisition of dynamic call information, we employ \textbf{Simpleperf}. Specifically, we sample execution at a frequency of 1000\,Hz and, to ensure uniform processing, record only the offsets of call sites rather than their symbolic names. The raw output from Simpleperf is a call stack, which we process by identifying contiguous sequences of frames belonging to the same binary module to derive \((cs, ct)\) pairs. We emphasize that while dynamic instrumentation tools like Frida could achieve the same outcome, our empirical evaluation revealed that Frida's instrumentation overhead introduced prohibitive application startup latencies, often causing the target process to hang or be terminated by the operating system.

The implementation of Executor is centered on \texttt{Frida}. We utilize its \texttt{Stalker} engine to trace instruction sequences and thereby capture execution information. Frida's hooking capabilities are instrumental for intercepting the target function, while its comprehensive API provides the necessary primitives for reading and writing register and memory state, as required by TG. A critical design choice is our use of an indirect triggering mechanism. Rather than invoking the target function directly---a precarious approach given that parameters can be exceedingly complex and involve pointer values that are impossible to synthesize correctly, leading to inevitable crashes---we instead hook the function and intercept it during a live execution. This strategy ensures that when our analysis begins, the function's parameters have already been populated with valid values by the program's natural control flow.

Our experiments were conducted on a server running Ubuntu 20.04, with the target device being an ARM-based machine running Android 15. For the control parameters used in Algorithm~\ref{alg:HyBinMAS}, we set the maximum iterations $M$ to 3, the maximum retries $R$ to 3, and the maximum number of test cases $K$ to 3.

\subsection{Baselines}

Within the domain of function name recovery, we select \textbf{SymGen} as our primary baseline, as it represents the current state-of-the-art (SOTA). We acknowledge that \textbf{FoC} is another recent SOTA method; however, its focus is exclusively on cryptographic functions, placing it outside the scope of our general-purpose evaluation. Additionally, considering the formidable capabilities demonstrated by general-purpose Large Language Models (LLMs) across a wide range of tasks, we also include \textbf{Deepseek} as a baseline. Prior to \textbf{SymGen}, research in this area predominantly framed function name recovery as a classification task. Therefore, we have selected \textbf{XFL} from this line of traditional research to serve as a representative method in our set of baselines.

\subsection{Dataset}

\subsubsection{General-Purpose Dataset}

Many studies in function name recovery utilize projects from the GNU ecosystem to construct dataset. For instance, \textbf{SymGen} manually compiled 33 GNU projects to construct their evaluation dataset. To ensure a fair and direct comparison with our baselines, we replicated the dataset construction methodology proposed by  \textbf{SymGen}. It is important to note that the original \textbf{SymGen} dataset is extensive, comprising a vast number of functions from both executable files and shared libraries. Constrained by the token consumption of the \textbf{HyBinMAS} framework, and to establish a level playing field with the dataset we introduce later, we elected to extract only the shared library files from their collection for RQ1. Furthermore, the original dataset provided by \textbf{SymGen} contains only decompiled code and lacks other artifacts required by \textbf{HyBinMAS} (such as assembly code and call graphs). We therefore processed the binaries to supplement this dataset with the necessary additional information. The statistical details of the resulting dataset are presented in Figure~\ref{fig:symgen_dataset_stats}.

\subsubsection{ARM64 Dataset}
\begin{figure}[h] % [h] 表示图片放置在当前位置
    \centering
    \includegraphics[width=0.5\textwidth]{image/ConstructDataset.drawio.png} % 调整图片宽度为文本宽度的50%
    \caption{An overview of the dataset construction process} % 添加图片标题
    \label{fig:construct-dataset} % 设置图片标签，方便引用
\end{figure}

To the best of our knowledge, no dedicated dataset currently exists for semantic understanding tasks on Android Native Libraries, such as function name recovery and comment generation. Although numerous data sets are available for stripped binaries,ies, they are inadequate for our purposes. These existing datasets either lack compilation variants for the ARM64 architecture or are not executable on Android devices, which is unacceptable for our method. Therefore, we are working on the construction of a new specialized dataset. The overview of this process is illustrated in Figure 3.

\textbf{1)Android Project Selection}. Our ultimate objective is to analyze closed-source applications. Therefore, the primary principle guiding our selection of open-source projects is to ensure they accurately reflect the characteristics of their closed-source counterparts. Inspired by BinMetric and BinaryLLM-Eval, we adapt and extend their critera to align with the specific requirements of our work:

\begin{itemize}
    \item \textbf{Executability:} The requirement for executability is twofold. The first consideration is practical applicability, necessitating that the target application be runnable. The second is the operational constraints imposed by the EA within our proposed approach.
    
    \item \textbf{Native Code  Availability:} Selected projects must contain native source code (implemented in C or C++). This is crucial as projects providing only pre-compiled .so files would prevent us from obtaining the ground-truth function semantics (i.e., names and comments). It is critical that the target functions must be non-exported. This ensures that their symbol information can be completely removed during the stripping process without impairing the application's runtime functionality—a task that is infeasible for exported functions.

    \item \textbf{Realisticity and Modernity:} To ensure the dataset reflects contemporary, real-world scenarios, the selected projects must be compatible with latest Android versions. At the time of our data collection, with Android 14+ gradually becoming the mainstream system, we established Android 15 compatibility as our target standard.

    \item \textbf{Project Quality:} High-quality projects are essential, as factors like code clarity and structure can significantly influence the performance of semantic analysis. We identified quality through characteristics such as a well-organized project structure, clear logical implementation, and adherence to good naming conventions. Project quality can be quantifiedby its Stars and Forks on GitHub.

    \item \textbf{Domain Diversity:} To comprehensively evaluate the generalization capabilities of our proposed method, the dataset should encompass applications from a broad range of domains. We identified and included projects from the following categories: Communication \& Social Networking,  Science \& Education, Multimedia, Game, Finance and Tools. It is important to acknowledge that these categories do not cover the full spectrum of real-world applications. Numerous projects from other domains were filtered out as their failure to meet the 1st criterion.

    \item \textbf{Maintenance Status:} To ensure the relevance of the compiled binaries and their underlying code, projects were required to have a recent history of updates or maintenance. We have chosen 5 years as the threshold.
\end{itemize}

From an initial survey of [Number] Android application projects, we downloaded [Number] candidates based on the requirements outlined above. During the subsequent compilation and build process, we filtered out any project that could not be constructed as specified (see details below), resulting in a final corpus of [Number] APKs. These artifacts are then subjected to the subsequent processing stages (Steps \normalsize{\textcircled{\scriptsize{3}}}\normalsize–\normalsize{\textcircled{\scriptsize{7}}}\normalsize) illustrated in Figure 3.


\textbf{2)APK Construction}. We utilized Gradle to complete the build tasks. For each application, we compiled two distinct variants: **Release** and **Release-DWARF**. It is important to note that we deliberately avoided the common practice of building a standard Debug version with DWARF information for its Release counterpart. This is because build tools like Gradle and the NDK apply different configurations (e.g., varying optimization levels) to Debug and Release builds. Such discrepancies will result in non-identical native code, thereby complicating the subsequent alignment process.

Instead, our approach uses the standard Release as the baseline. We generate the **Release-DWARF** variant by augmenting the Release binaries with DWARF information while ensuring the underlying machine code remained unchanged. The final executable deployed on the Android system is also the standard Release, which perfectly aligns with real-world scenarios. Consistent with established practices in previous research, we apply Step \normalsize{\textcircled{\scriptsize{3}}}\normalsize, employing Ghidra to generate the corresponding disassembly and pseudo-C code for these binaries.


\textbf{3)Ground-Truth Construction}. This process begins with the extraction of native source code from the selected projects (as shown in step \normalsize{\textcircled{\scriptsize{4}}}\normalsize).
\begin{itemize}
    \item For the \textbf{Function Name Recovery (FNR)} task, we designated the developer-written function names as the ground-truth. This was accomplished by parsing the C/C++ source code using tree-sitter (\normalsize{\textcircled{\scriptsize{6}}}\normalsize).
    \item For the \textbf{Binary Summarization} task, we opted \textbf{NOT} to use the comments present in the source code. As demonstrated by previous research, these comments often suffer from low quality and incompletion. Conversely, a significant body of work has demonstrated that LLMs, exemplified by GPT, can generate high-quality, descriptive summaries—often surpassing the quality of those written by human developers. Therefore, we employed an LLM to perform this summarization task (\normalsize{\textcircled{\scriptsize{5}}}\normalsize).
\end{itemize}


\textbf{4)Alignment}. In step \circlenum{2}, we obtained the binary variant that includes DWARF information. This variant shares identical function address offsets with its stripped counterpart, which allows for a straightforward alignment of the function body with their corresponding function name. Furthermore, the DWARF data contains precise records of the source code locations (i.e., file and line number) for each function and variable. This information serves as the definitive basis for aligning the decompiled pseudo-C code with the original source code. Through this two-step alignment process, we can reliably link each function in the binary (represented by both its assembly and pseudo-C code) to its corresponding ground-truth.


\textbf{5)Filtering and Leakage Check}. To accurately simulate the senarios of real-world applications, we implemented two filtering steps. First, we filtered out any functions that retained their symbol information post-stripping to avoid overestimating the method's performance. Second, we discarded functions with very small bodies (defined as having fewer than 5 assembly instructions).

Performing a data leakage check on the pseudo-C code within our dataset is a crucial final step. This is to ensure that the data has not been seen by the LLM during its pre-training phase. Following the methodology of existing research in related fields, we conducted this check by tasking an LLM with a pseudo-C code completion challenge. We observed that the average similarity between the LLM's auto-completed code and the original pseudo-C code was merely xxx-. This low score indicates that our dataset successfully passes the leakage check, confirming its novelty and suitability for training and evaluation.

Ultimately, we get xxx Android projects for our dataset. Detailed information for each project is presented in Table 1.



\subsection{Research Questions}



\section{Threats to Validity}




\section{Related Work}
\subsection{Symbol Recovery}

The strip command, a standard utility in toolchains, removes symbol information from binaries, including variable names and type definitions. The absence of this information significantly impedes the comprehension and analysis of binaries. Consequently, a substantial body of research has been dedicated to recovering these symbols. Despite pioneering efforts in this domain (Debin, DIRE, NERO, NFRE, Punstrip, and DIRTY), further research is needed to advance the symbol recovery task.

One major line of research focuses on the recovery of variables and data structures, such as (TypeFSL, ReSym, HyRES, TypeForge). Another significant research direction, which aligns with one of the primary goals of this paper, is the recovery of function names. Meaningful function names provide a global summary of a subroutine's purpose, making them invaluable for program understanding. Recent works such as SymLM and ASMDepictor have leveraged the powerful encoding capabilities of Transformers to convert assembly code into semantic embeddings for predicting function names. To address the practical challenge of out-of-vocabulary (OOV) function names, XFL models the task as a multi-label classification problem, predicting each sub-token of a function name individually. More innovatively, SymGen reformulates function name recovery as a generative task and trains a bespoke LLM for this purpose. This approach effectively resolves the poor generalization caused by the OOV problem in previous methods and achieves results that significantly surpass prior work.

While our methodology shares conceptual similarities with Sym-LM's use of dynamic analysis, our approach orchestrates a Multi-Agent system for collaborative reasoning and inference, allowing for a more effective use of dynamic information.

\subsection{Code Summarization}
\textbf{Code Summarization}, the task of generating concise natural language descriptions for a given piece of code, is indispensable for program comprehension. The field has primarily focused on source code summarization, for which a wide array of techniques have been developed. These methodologies have evolved from early template-based and information retrieval (IR) approaches to more sophisticated techniques grounded in deep learning. Most recently, the advent of Large Language Models (LLMs) has marked a paradigm shift. LLM-based methods are now considered the state-of-the-art, with studies showing their generated summaries can sometimes even surpass the quality of human-written reference comments.

However, a considerable semantic gap persists when applying LLMs to binary code, as these models often struggle to understand low-level machine instructions without symbolic information. Although some researchers have begun to explore LLM-based binary code summarization, as exemplified by work such as FoC, these pioneering efforts predominantly rely on fine-tuning bespoke models for this specific task. Inspired by the remarkable performance of Multi-Agent systems in solving complex reasoning tasks in other domains, our method leverages a Multi-Agent system to collaboratively guide and enhance an LLM's comprehension and reasoning capabilities over binary code.


\subsection{LLM for Binaries}
LLMs have demonstrated remarkable success across a spectrum of source code-related tasks (). As LLM technology continues to evolve, a growing body of research has started to explore its application in the binary analysis domain, including symbol recovery (e.g., ReSym, HyRES, SymGen) and the improvement of decompilation output (e.g., LLM4Decompile, DeGPT).

However, for practitioners in the field, such as reverse engineers, a more fundamental challenge lies in the semantic comprehension of the binary code itself. FoC begins to address this critical need, but its approach has two significant limitations: its applicability is confined to the specific domain of cryptographic functions, and it necessitates the fine-tuning of a dedicated LLM. In stark contrast, our method is designed to be domain-agnostic, imposing no restrictions on the type of binary, and obviating the significant computational and data overhead associated with model training.



\section{Conclusion}




%%
%% The acknowledgments section is defined using the "acks" environment
%% (and NOT an unnumbered section). This ensures the proper
%% identification of the section in the article metadata, and the
%% consistent spelling of the heading.
\begin{acks}
To Robert, for the bagels and explaining CMYK and color spaces.
\end{acks}

%%
%% The next two lines define the bibliography style to be used, and
%% the bibliography file.
\bibliographystyle{ACM-Reference-Format}
\bibliography{sample-base}


%%
%% If your work has an appendix, this is the place to put it.
\appendix

% \section{Research Methods}

% \subsection{Part One}

% Lorem ipsum dolor sit amet, consectetur adipiscing elit. Morbi
% malesuada, quam in pulvinar varius, metus nunc fermentum urna, id
% sollicitudin purus odio sit amet enim. Aliquam ullamcorper eu ipsum
% vel mollis. Curabitur quis dictum nisl. Phasellus vel semper risus, et
% lacinia dolor. Integer ultricies commodo sem nec semper.

% \subsection{Part Two}

% Etiam commodo feugiat nisl pulvinar pellentesque. Etiam auctor sodales
% ligula, non varius nibh pulvinar semper. Suspendisse nec lectus non
% ipsum convallis congue hendrerit vitae sapien. Donec at laoreet
% eros. Vivamus non purus placerat, scelerisque diam eu, cursus
% ante. Etiam aliquam tortor auctor efficitur mattis.

% \section{Online Resources}

% Nam id fermentum dui. Suspendisse sagittis tortor a nulla mollis, in
% pulvinar ex pretium. Sed interdum orci quis metus euismod, et sagittis
% enim maximus. Vestibulum gravida massa ut felis suscipit
% congue. Quisque mattis elit a risus ultrices commodo venenatis eget
% dui. Etiam sagittis eleifend elementum.

% Nam interdum magna at lectus dignissim, ac dignissim lorem
% rhoncus. Maecenas eu arcu ac neque placerat aliquam. Nunc pulvinar
% massa et mattis lacinia.

\end{document}
\endinput
%%
%% End of file `sample-sigconf-authordraft.tex'.

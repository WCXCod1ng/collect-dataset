
@misc{LLM_cs,
	title = {Large {Language} {Models} for {Code} {Summarization}},
	url = {http://arxiv.org/abs/2405.19032},
	doi = {10.48550/arXiv.2405.19032},
	abstract = {Recently, there has been increasing activity in using deep learning for software engineering, including tasks like code generation and summarization. In particular, the most recent coding Large Language Models seem to perform well on these problems. In this technical report, we aim to review how these models perform in code explanation/summarization, while also investigating their code generation capabilities (based on natural language descriptions).},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Szalontai, Balázs and Szalay, Gergő and Márton, Tamás and Sike, Anna and Pintér, Balázs and Gregorics, Tibor},
	month = may,
	year = {2024},
	note = {arXiv:2405.19032 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Programming Languages, Computer Science - Software Engineering},
	annote = {Comment: technical report with 11 pages, 1 figure, 10 tables},
	annote = {LLM\_cs
},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\VFYJ5CKH\\Szalontai 等 - 2024 - Large Language Models for Code Summarization.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\G2V8A3CR\\2405.html:text/html},
}

@misc{scs_via_LLM,
	title = {Source {Code} {Summarization} in the {Era} of {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2407.07959},
	doi = {10.48550/arXiv.2407.07959},
	abstract = {To support software developers in understanding and maintaining programs, various automatic (source) code summarization techniques have been proposed to generate a concise natural language summary (i.e., comment) for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of code-related tasks. In this paper, we undertake a systematic and comprehensive study on code summarization in the era of LLMs, which covers multiple aspects involved in the workflow of LLM-based code summarization. Specifically, we begin by examining prevalent automated evaluation methods for assessing the quality of summaries generated by LLMs and find that the results of the GPT-4 evaluation method are most closely aligned with human evaluation. Then, we explore the effectiveness of five prompting techniques (zero-shot, few-shot, chain-of-thought, critique, and expert) in adapting LLMs to code summarization tasks. Contrary to expectations, advanced prompting techniques may not outperform simple zero-shot prompting. Next, we investigate the impact of LLMs' model settings (including top{\textbackslash}\_p and temperature parameters) on the quality of generated summaries. We find the impact of the two parameters on summary quality varies by the base LLM and programming language, but their impacts are similar. Moreover, we canvass LLMs' abilities to summarize code snippets in distinct types of programming languages. The results reveal that LLMs perform suboptimally when summarizing code written in logic programming languages compared to other language types. Finally, we unexpectedly find that CodeLlama-Instruct with 7B parameters can outperform advanced GPT-4 in generating summaries describing code implementation details and asserting code properties. We hope that our findings can provide a comprehensive understanding of code summarization in the era of LLMs.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Sun, Weisong and Miao, Yun and Li, Yuekang and Zhang, Hongyu and Fang, Chunrong and Liu, Yi and Deng, Gelei and Liu, Yang and Chen, Zhenyu},
	month = jul,
	year = {2024},
	note = {arXiv:2407.07959 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	annote = {Comment: Just accepted to the 47th International Conference on Software Engineering (ICSE 2025)},
	annote = {scs\_via\_LLM
},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\5NN6AWR3\\Sun 等 - 2024 - Source Code Summarization in the Era of Large Language Models.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\SPCX5VQT\\2407.html:text/html},
}

@misc{cs_chatgpt,
	title = {Automatic {Code} {Summarization} via {ChatGPT}: {How} {Far} {Are} {We}?},
	shorttitle = {Automatic {Code} {Summarization} via {ChatGPT}},
	url = {http://arxiv.org/abs/2305.12865},
	doi = {10.48550/arXiv.2305.12865},
	abstract = {To support software developers in understanding and maintaining programs, various automatic code summarization techniques have been proposed to generate a concise natural language comment for a given code snippet. Recently, the emergence of large language models (LLMs) has led to a great boost in the performance of natural language processing tasks. Among them, ChatGPT is the most popular one which has attracted wide attention from the software engineering community. However, it still remains unclear how ChatGPT performs in (automatic) code summarization. Therefore, in this paper, we focus on evaluating ChatGPT on a widely-used Python dataset called CSN-Python and comparing it with several state-of-the-art (SOTA) code summarization models. Specifically, we first explore an appropriate prompt to guide ChatGPT to generate in-distribution comments. Then, we use such a prompt to ask ChatGPT to generate comments for all code snippets in the CSN-Python test set. We adopt three widely-used metrics (including BLEU, METEOR, and ROUGE-L) to measure the quality of the comments generated by ChatGPT and SOTA models (including NCS, CodeBERT, and CodeT5). The experimental results show that in terms of BLEU and ROUGE-L, ChatGPT's code summarization performance is significantly worse than all three SOTA models. We also present some cases and discuss the advantages and disadvantages of ChatGPT in code summarization. Based on the findings, we outline several open challenges and opportunities in ChatGPT-based code summarization.},
	urldate = {2025-02-09},
	publisher = {arXiv},
	author = {Sun, Weisong and Fang, Chunrong and You, Yudu and Miao, Yun and Liu, Yi and Li, Yuekang and Deng, Gelei and Huang, Shenghan and Chen, Yuchen and Zhang, Quanjun and Qian, Hanwei and Liu, Yang and Chen, Zhenyu},
	month = may,
	year = {2023},
	note = {arXiv:2305.12865 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Software Engineering},
	annote = {cs\_chatgpt

},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\68Q7DTZI\\Sun 等 - 2023 - Automatic Code Summarization via ChatGPT How Far Are We.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\J4JL6LFW\\2305.html:text/html},
}

@inproceedings{statement_cs_LLM,
	title = {On the effectiveness of large language models in statement-level code summarization},
	url = {https://ieeexplore.ieee.org/abstract/document/10684656/},
	urldate = {2025-02-11},
	booktitle = {2024 {IEEE} 24th {International} {Conference} on {Software} {Quality}, {Reliability} and {Security} ({QRS})},
	publisher = {IEEE},
	author = {Zhu, Jie and Miao, Yun and Xu, Tingting and Zhu, Junwu and Sun, Xiaolei},
	year = {2024},
	pages = {216--227},
	annote = {statement\_cs\_LLM
},
}

@article{multi_intent_LLM,
	title = {Multi-{Intent} {Inline} {Code} {Comment} {Generation} via {Large} {Language} {Model}},
	volume = {34},
	url = {https://www.researchgate.net/profile/Zhang-Xiaowei-13/publication/379302180_Multi-Intent_Inline_Code_Comment_Generation_via_Large_Language_Model/links/6642c6097091b94e9326efbe/Multi-Intent-Inline-Code-Comment-Generation-via-Large-Language-Model.pdf},
	number = {06},
	urldate = {2025-02-24},
	journal = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Zhang, Xiaowei and Chen, Zhifei and Cao, Yulu and Chen, Lin and Zhou, Yuming},
	year = {2024},
	note = {Publisher: World Scientific Publishing Company},
	pages = {845--868},
	annote = {multi\_intent\_LLM
},
}

@misc{cs_beyond_function,
	title = {Code {Summarization} {Beyond} {Function} {Level}},
	url = {http://arxiv.org/abs/2502.16704},
	doi = {10.48550/arXiv.2502.16704},
	abstract = {Code summarization is a critical task in natural language processing and software engineering, which aims to generate concise descriptions of source code. Recent advancements have improved the quality of these summaries, enhancing code readability and maintainability. However, the content of a repository or a class has not been considered in function code summarization. This study investigated the effectiveness of code summarization models beyond the function level, exploring the impact of class and repository contexts on the summary quality. The study involved revising benchmarks for evaluating models at class and repository levels, assessing baseline models, and evaluating LLMs with in-context learning to determine the enhancement of summary quality with additional context. The findings revealed that the fine-tuned state-of-the-art CodeT5+ base model excelled in code summarization, while incorporating few-shot learning and retrieved code chunks from RAG significantly enhanced the performance of LLMs in this task. Notably, the Deepseek Coder 1.3B and Starcoder2 15B models demonstrated substantial improvements in metrics such as BLEURT, METEOR, and BLEU4 at both class and repository levels. Repository-level summarization exhibited promising potential but necessitates significant computational resources and gains from the inclusion of structured context. Lastly, we employed the recent SIDE code summarization metric in our evaluation. This study contributes to refining strategies for prompt engineering, few-shot learning, and RAG, addressing gaps in benchmarks for code summarization at various levels. Finally, we publish all study details, code, datasets, and results of evaluation in the GitHub repository available at https://github.com/kilimanj4r0/ code-summarization-beyond-function-level.},
	language = {en},
	urldate = {2025-04-12},
	publisher = {arXiv},
	author = {Makharev, Vladimir and Ivanov, Vladimir},
	month = feb,
	year = {2025},
	note = {arXiv:2502.16704 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	annote = {Comment: Accepted to LLM4Code @ ICSE'25; 8 pages, 3 figures, 4 tables},
	annote = {cs\_beyond\_function
},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\FHKBF3LP\\Makharev和Ivanov - 2025 - Code Summarization Beyond Function Level.pdf:application/pdf},
}

@article{structured_cs_hyctx,
	title = {Learning to {Generate} {Structured} {Code} {Summaries} {From} {Hybrid} {Code} {Context}},
	volume = {50},
	issn = {1939-3520},
	url = {https://ieeexplore.ieee.org/document/10636040/},
	doi = {10.1109/TSE.2024.3439562},
	abstract = {Code summarization aims to automatically generate natural language descriptions for code, and has become a rapidly expanding research area in the past decades. Unfortunately, existing approaches mainly focus on the “one-to-one” mapping from methods to short descriptions, which hinders them from becoming practical tools: 1) The program context is ignored, so they have difficulty in predicting keywords outside the target method; 2) They are typically trained to generate brief function descriptions with only one sentence in length, and therefore have difficulty in providing specific information. These drawbacks are partially due to the limitations of public code summarization datasets. In this paper, we first build a large code summarization dataset including different code contexts and summary content annotations, and then propose a deep learning framework that learns to generate structured code summaries from hybrid program context, named StructCodeSum. It provides both an LLM-based approach and a lightweight approach which are suitable for different scenarios. Given a target method, StructCodeSum predicts its function description, return description, parameter description, and usage description through hybrid code context, and ultimately builds a Javadoc-style code summary. The hybrid code context consists of path context, class context, documentation context and call context of the target method. Extensive experimental results demonstrate: 1) The hybrid context covers more than 70\% of the summary tokens in average and significantly boosts the model performance; 2) When generating function descriptions, StructCodeSum outperforms the state-of-the-art approaches by a large margin; 3) According to human evaluation, the quality of the structured summaries generated by our approach is better than the documentation generated by Code Llama.},
	number = {10},
	urldate = {2025-04-13},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhou, Ziyi and Li, Mingchen and Yu, Huiqun and Fan, Guisheng and Yang, Penghui and Huang, Zijie},
	month = oct,
	year = {2024},
	keywords = {Codes, Task analysis, Source coding, program comprehension, Code summarization, Documentation, Annotations, Context modeling, deep learning, Hybrid power systems},
	pages = {2512--2528},
	annote = {structured\_cs\_hyctx
},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\MBD5F8UA\\Zhou 等 - 2024 - Learning to Generate Structured Code Summaries From Hybrid Code Context.pdf:application/pdf},
}

@article{HyRes,
	title = {{HyRES}: {Recovering} {Data} {Structures} in {Binaries} via {Semantic} {Enhanced} {Hybrid} {Reasoning}},
	issn = {1049-331X, 1557-7392},
	shorttitle = {{HyRES}},
	url = {https://dl.acm.org/doi/10.1145/3736719},
	doi = {10.1145/3736719},
	abstract = {Binary reverse engineering is pivotal in the realm of cybersecurity, enabling critical applications such as malware analysis, legacy code hardening, and vulnerability detection. However, the challenge of recovering structural information from binaries, especially stripped ones, persists due to the significant loss of variable boundaries, types, names and data flow information during compilation. In this paper, we introduce HyRES (Hybrid REasoning For Structure Recovery), an innovative hybrid reasoning technique that energizes static analysis, large language model (LLM), and heuristic methods to recover data structures from stripped binaries. It analyzes the structure layout and proficiently infer its semantics via LLM, and utilizes semantics to perform semantic-enhanced structure aggregation, which overcomes the need for complete data flow. HyRES outperforms state-of-the-art (SOTA) solutions in terms of structure pointer identification and layout recovery. Specifically, HyRES achieves 65.1\% higher recall and 33.4\% higher accuracy than the SOTA, while also being 64.2\% faster than existing SOTA solutions. Comprehensive experiments demonstrate HyRES’s superior performance and practical utility in real-world reverse engineering tasks, marking a significant advancement in binary analysis. CCS Concepts: • Computing methodologies → Machine learning; • Security and privacy → Software reverse engineering.},
	language = {en},
	urldate = {2025-07-12},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Sha, Zihan and Shu, Hui and Wang, Hao and Gao, Zeyu and Lan, Yang and Zhang, Chao},
	month = may,
	year = {2025},
	note = {Publisher: Association for Computing Machinery (ACM)},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\E2EE7DJF\\Sha 等 - 2025 - HyRES Recovering Data Structures in Binaries via Semantic Enhanced Hybrid Reasoning.pdf:application/pdf},
}

@inproceedings{ReSym,
	address = {Salt Lake City UT USA},
	title = {{ReSym}: {Harnessing} {LLMs} to {Recover} {Variable} and {Data} {Structure} {Symbols} from {Stripped} {Binaries}},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
	shorttitle = {{ReSym}},
	url = {https://dl.acm.org/doi/10.1145/3658644.3670340},
	doi = {10.1145/3658644.3670340},
	abstract = {Decompilation aims to recover a binary executable to the source code form and hence has a wide range of applications in cyber security, such as malware analysis and legacy code hardening. A prominent challenge is to recover variable symbols, including both primitive and complex types such as user-defined data structures, along with their symbol information such as names and types. Existing efforts focus on solving parts of the problem, e.g., recovering only types (without names) or only local variables (without user-defined structures). In this paper, we propose ReSym, a novel hybrid technique that combines Large Language Models (LLMs) and program analysis to recover both names and types for local variables and user-defined data structures. Our method encompasses fine-tuning two LLMs to handle local variables and structures, respectively. To overcome the inherent token limitations in current LLMs, we devise a novel Prolog-based algorithm to aggregate and cross-check results from multiple LLM queries, suppressing uncertainty and hallucinations. Our experiments show that ReSym is effective in recovering variable information and user-defined data structures, substantially outperforming the state-of-the-art methods.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 2024 on {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Xie, Danning and Zhang, Zhuo and Jiang, Nan and Xu, Xiangzhe and Tan, Lin and Zhang, Xiangyu},
	month = dec,
	year = {2024},
	pages = {4554--4568},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\MTQ2JBSQ\\Xie 等 - 2024 - ReSym Harnessing LLMs to Recover Variable and Data Structure Symbols from Stripped Binaries.pdf:application/pdf},
}

@inproceedings{SymLM,
	address = {Los Angeles CA USA},
	title = {{SymLM}: {Predicting} {Function} {Names} in {Stripped} {Binaries} via {Context}-{Sensitive} {Execution}-{Aware} {Code} {Embeddings}},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	shorttitle = {{SymLM}},
	url = {https://dl.acm.org/doi/10.1145/3548606.3560612},
	doi = {10.1145/3548606.3560612},
	abstract = {Predicting function names in stripped binaries is an extremely useful but challenging task, as it requires summarizing the execution behavior and semantics of the function in human languages. Recently, there has been significant progress in this direction with machine learning. However, existing approaches fail to model the exhaustive function behavior and thus suffer from the poor generalizability to unseen binaries. To advance the state of the art, we present a function Symbol name prediction and binary Language Modeling (SymLM) framework, with a novel neural architecture that learns the comprehensive function semantics by jointly modeling the execution behavior of the calling context and instructions via a novel fusing encoder. We have evaluated SymLM with 1,431,169 binary functions from 27 popular open source projects, compiled with 4 optimizations (O0-O3) for 4 different architectures (i.e., x64, x86, ARM, and MIPS) and 4 obfuscations. SymLM outperforms the stateof-the-art function name prediction tools by up to 15.4\%, 59.6\%, and 35.0\% in precision, recall, and F1 score, with significantly better generalizability and obfuscation resistance. Ablation studies also show that our design choices (e.g., fusing components of the calling context and execution behavior) substantially boost the performance of function name prediction. Finally, our case studies further demonstrate the practical use cases of SymLM in analyzing firmware images.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 2022 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Jin, Xin and Pei, Kexin and Won, Jun Yeon and Lin, Zhiqiang},
	month = nov,
	year = {2022},
	pages = {1631--1645},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\QEFSAUYN\\Jin 等 - 2022 - SymLM Predicting Function Names in Stripped Binaries via Context-Sensitive Execution-Aware Code Emb.pdf:application/pdf},
}

@inproceedings{XFL,
	title = {{XFL}: {Naming} {Functions} in {Binaries} with {Extreme} {Multi}-label {Learning}},
	shorttitle = {{XFL}},
	url = {https://ieeexplore.ieee.org/document/10179439/},
	doi = {10.1109/SP46215.2023.10179439},
	abstract = {Reverse engineers benefit from the presence of identifiers such as function names in a binary, but usually these are removed for release. Training a machine learning model to predict function names automatically is promising but fundamentally hard: unlike words in natural language, most function names occur only once. In this paper, we address this problem by introducing eXtreme Function Labeling (XFL), an extreme multi-label learning approach to selecting appropriate labels for binary functions. XFL splits function names into tokens, treating each as an informative label akin to the problem of tagging texts in natural language. We relate the semantics of binary code to labels through Dexter, a novel function embedding that combines static analysis-based features with local context from the call graph and global context from the entire binary. We demonstrate that XFL/Dexter outperforms the state of the art in function labeling on a dataset of 10,047 binaries from the Debian project, achieving a precision of 83.5\%. We also study combinations of XFL with alternative binary embeddings from the literature and show that Dexter consistently performs best for this task. As a result, we demonstrate that binary function labeling can be effectively phrased in terms of multi-label learning, and that binary function embeddings benefit from including explicit semantic features.},
	urldate = {2025-07-12},
	booktitle = {2023 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Patrick-Evans, James and Dannehl, Moritz and Kinder, Johannes},
	month = may,
	year = {2023},
	note = {ISSN: 2375-1207},
	keywords = {Training, Semantics, Static analysis, Natural languages, Predictive models, Binary codes, Binary-Analysis, Extreme-Multi-label-Learning, Representation-Learning, Reverse-Engineering, XML},
	pages = {2375--2390},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\EPLLN58Z\\Patrick-Evans 等 - 2023 - XFL Naming Functions in Binaries with Extreme Multi-label Learning.pdf:application/pdf},
}

@inproceedings{SymGen,
	address = {San Diego, CA, USA},
	title = {Beyond {Classification}: {Inferring} {Function} {Names} in {Stripped} {Binaries} via {Domain} {Adapted} {LLMs}},
	shorttitle = {Beyond {Classification}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2025-797-paper.pdf},
	doi = {10.14722/ndss.2025.240797},
	abstract = {Function name inference in stripped binaries is an important yet challenging task for many security applications, such as malware analysis and vulnerability discovery, due to the need to grasp binary code semantics amidst diverse instruction sets, architectures, compiler optimizations, and obfuscations. While machine learning has made significant progress in this field, existing methods often struggle with unseen data, constrained by their reliance on a limited vocabulary-based classification approach. In this paper, we present SYMGEN, a novel framework employing an autoregressive generation paradigm powered by domain-adapted generative large language models (LLMs) for enhanced binary code interpretation. We have evaluated SYMGEN on a dataset comprising 2,237,915 binary functions across four architectures (x86-64, x86-32, ARM, MIPS) with four levels of optimizations (O0-O3) where it surpasses the state-of-the-art with up to 409.3\%, 553.5\%, and 489.4\% advancement in precision, recall, and F1 score, respectively, showing superior effectiveness and generalizability. Our ablation and case studies also demonstrate the significant performance boosts achieved by our design, e.g., the domain adaptation approach, alongside showcasing SYMGEN’s practicality in analyzing real-world binaries, e.g., obfuscated binaries and malware executables.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings 2025 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Jiang, Linxi and Jin, Xin and Lin, Zhiqiang},
	year = {2025},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\TC9ZY8JP\\Jiang 等 - 2025 - Beyond Classification Inferring Function Names in Stripped Binaries via Domain Adapted LLMs.pdf:application/pdf},
}

@inproceedings{AsmDepictor,
	address = {Melbourne VIC Australia},
	title = {A {Transformer}-based {Function} {Symbol} {Name} {Inference} {Model} from an {Assembly} {Language} for {Binary} {Reversing}},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	url = {https://dl.acm.org/doi/10.1145/3579856.3582823},
	doi = {10.1145/3579856.3582823},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the {ACM} {Asia} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Kim, Hyunjin and Bak, Jinyeong and Cho, Kyunghyun and Koo, Hyungjoon},
	month = jul,
	year = {2023},
	pages = {951--965},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\9IRV57M9\\Kim 等 - 2023 - A Transformer-based Function Symbol Name Inference Model from an Assembly Language for Binary Revers.pdf:application/pdf},
}

@inproceedings{DeBin,
	address = {Toronto Canada},
	title = {Debin: {Predicting} {Debug} {Information} in {Stripped} {Binaries}},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	shorttitle = {Debin},
	url = {https://dl.acm.org/doi/10.1145/3243734.3243866},
	doi = {10.1145/3243734.3243866},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 2018 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {He, Jingxuan and Ivanov, Pesho and Tsankov, Petar and Raychev, Veselin and Vechev, Martin},
	month = oct,
	year = {2018},
	pages = {1667--1680},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\G39XITGM\\He 等 - 2018 - Debin Predicting Debug Information in Stripped Binaries.pdf:application/pdf},
}

@article{NERO,
	title = {Neural reverse engineering of stripped binaries using augmented control flow graphs},
	volume = {4},
	copyright = {https://creativecommons.org/licenses/by-nc-sa/4.0/},
	issn = {2475-1421},
	url = {https://dl.acm.org/doi/10.1145/3428293},
	doi = {10.1145/3428293},
	abstract = {We address the problem of reverse engineering of stripped executables, which contain no debug information. This is a challenging problem because of the low amount of syntactic information available in stripped executables, and the diverse assembly code patterns arising from compiler optimizations. We present a novel approach for predicting procedure names in stripped executables. Our approach combines static analysis with neural models. The main idea is to use static analysis to obtain augmented representations of call sites; encode the structure of these call sites using the control-flow graph (CFG) and finally, generate a target name while attending to these call sites. We use our representation to drive graph-based, LSTM-based and Transformer-based architectures. Our evaluation shows that our models produce predictions that are difficult and time consuming for humans, while improving on existing methods by 28\% and by 100\% over state-of-the-art neural textual models that do not use any static analysis. Code and data for this evaluation are available at https://github.com/tech-srl/Nero.},
	language = {en},
	number = {OOPSLA},
	urldate = {2025-07-12},
	journal = {Proceedings of the ACM on Programming Languages},
	author = {David, Yaniv and Alon, Uri and Yahav, Eran},
	month = nov,
	year = {2020},
	note = {Publisher: Association for Computing Machinery (ACM)},
	pages = {1--28},
	annote = {NERO
},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\QNR58MQ3\\David 等 - 2020 - Neural reverse engineering of stripped binaries using augmented control flow graphs.pdf:application/pdf},
}

@inproceedings{DIRE,
	title = {{DIRE}: {A} {Neural} {Approach} to {Decompiled} {Identifier} {Naming}},
	shorttitle = {{DIRE}},
	url = {https://ieeexplore.ieee.org/abstract/document/8952404},
	doi = {10.1109/ASE.2019.00064},
	abstract = {The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3\% of the time.},
	urldate = {2025-07-12},
	booktitle = {2019 34th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	author = {Lacomis, Jeremy and Yin, Pengcheng and Schwartz, Edward and Allamanis, Miltiadis and Le Goues, Claire and Neubig, Graham and Vasilescu, Bogdan},
	month = nov,
	year = {2019},
	note = {ISSN: 2643-1572},
	keywords = {Training, Software, Analytical models, Deep learning, Tools, Reverse engineering, Decompilation, Recurrent neural networks},
	pages = {628--639},
	file = {已提交版本:D\:\\tools\\Zotero\\Volume\\storage\\PN2CJ5BR\\Lacomis 等 - 2019 - DIRE A Neural Approach to Decompiled Identifier Naming.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\WPT3X8MG\\8952404.html:text/html},
}

@inproceedings{patrick-evans_probabilistic_2020,
	address = {Austin USA},
	title = {Probabilistic {Naming} of {Functions} in {Stripped} {Binaries}},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	url = {https://dl.acm.org/doi/10.1145/3427228.3427265},
	doi = {10.1145/3427228.3427265},
	urldate = {2025-07-12},
	booktitle = {Annual {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Patrick-Evans, James and Cavallaro, Lorenzo and Kinder, Johannes},
	month = dec,
	year = {2020},
	pages = {373--385},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\AZFQWV7A\\Patrick-Evans 等 - 2020 - Probabilistic Naming of Functions in Stripped Binaries.pdf:application/pdf},
}

@inproceedings{HexT5,
	address = {Luxembourg, Luxembourg},
	title = {{HexT5}: {Unified} {Pre}-{Training} for {Stripped} {Binary} {Code} {Information} {Inference}},
	copyright = {https://doi.org/10.15223/policy-029},
	shorttitle = {{HexT5}},
	url = {https://ieeexplore.ieee.org/document/10298504/},
	doi = {10.1109/ase56229.2023.00099},
	abstract = {Decompilation is a widely used process for reverse engineers to signiﬁcantly enhance code readability by lifting assembly code to a higher-level C-like language, pseudo-code. Nevertheless, the process of compilation and stripping irreversibly discards high-level semantic information that is crucial to code comprehension, such as comments, identiﬁer names, and types. Existing approaches typically recover only one type of information, making them suboptimal for semantic inference. In this paper, we treat pseudo-code as a special programming language, then present a uniﬁed pre-trained model, HexT5, that is trained on vast amounts of natural language comments, source identiﬁers, and pseudo-code using novel pseudo-code-based pretraining objectives. We ﬁne-tune HexT5 on various downstream tasks, including code summarization, variable name recovery, function name recovery, and similarity detection. Comprehensive experiments show that HexT5 achieves state-of-the-art performance on four downstream tasks, and it demonstrates the robust effectiveness and generalizability of HexT5 for binary-related tasks.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {2023 38th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering} ({ASE})},
	publisher = {IEEE},
	author = {Xiong, Jiaqi and Chen, Guoqiang and Chen, Kejiang and Gao, Han and Cheng, Shaoyin and Zhang, Weiming},
	month = sep,
	year = {2023},
	pages = {774--786},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\FGLZKEI7\\Xiong 等 - 2023 - HexT5 Unified Pre-Training for Stripped Binary Code Information Inference.pdf:application/pdf},
}

@inproceedings{chen_augmenting_2022,
	title = {Augmenting {Decompiler} {Output} with {Learned} {Variable} {Names} and {Types}},
	isbn = {978-1-939133-31-1},
	url = {https://www.usenix.org/conference/usenixsecurity22/presentation/chen-qibin},
	language = {en},
	urldate = {2025-07-12},
	author = {Chen, Qibin and Lacomis, Jeremy and Schwartz, Edward J. and Goues, Claire Le and Neubig, Graham and Vasilescu, Bogdan},
	year = {2022},
	pages = {4327--4343},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\URD7ZU77\\Chen 等 - 2022 - Augmenting Decompiler Output with Learned Variable Names and Types.pdf:application/pdf},
}

@inproceedings{NFRE,
	address = {Virtual Denmark},
	title = {A lightweight framework for function name reassignment based on large-scale stripped binaries},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	url = {https://dl.acm.org/doi/10.1145/3460319.3464804},
	doi = {10.1145/3460319.3464804},
	abstract = {Software in the wild is usually released as stripped binaries that contain no debug information (e.g., function names). This paper studies the issue of reassigning descriptive names for functions to help facilitate reverse engineering. Since the essence of this issue is a data-driven prediction task, persuasive research should be based on sufficiently large-scale and diverse data. However, prior studies can only be based on small-scale datasets because their techniques suffer from heavyweight binary analysis, making them powerless in the face of big-size and large-scale binaries.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	publisher = {ACM},
	author = {Gao, Han and Cheng, Shaoyin and Xue, Yinxing and Zhang, Weiming},
	month = jul,
	year = {2021},
	pages = {607--619},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\UVZYDCY8\\Gao 等 - 2021 - A lightweight framework for function name reassignment based on large-scale stripped binaries.pdf:application/pdf},
}

@inproceedings{ye_manta_2024,
	address = {Hilton La Jolla Torrey Pines La Jolla CA USA},
	title = {Manta: {Hybrid}-{Sensitive} {Type} {Inference} {Toward} {Type}-{Assisted} {Bug} {Detection} for {Stripped} {Binaries}},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	shorttitle = {Manta},
	url = {https://dl.acm.org/doi/10.1145/3622781.3674177},
	doi = {10.1145/3622781.3674177},
	abstract = {Static binary bug detection has been a prominent approach for ensuring the security of binaries used in our daily lives. However, the type information lost in binaries prevents the improvement opportunity for a static analyzer to utilize type information to prune away infeasible facts and increase analysis precision. To make binary bug detection more practical with higher precision, in this work, we propose the first hybrid-sensitive type inference, Manta, that combines dataflow analysis with different sensitivities to complement each other and infer precise types for many variables. The inferred types are then used to assist with bug detection by pruning infeasible indirect call targets and data dependencies. Our experiments indicate Manta outperforms prior work by inferring types with 78.7\% precision and 97.2\% recall. Based on the inferred types, we can prune away 63.9\% more infeasible indirect-call targets compared to existing type analysis techniques and perform program slicing on binaries with 61.1\% similarity to that on source code. Moreover, Manta has led to 86 new developer-confirmed vulnerabilities in many popular IoT firmware, with 64 CVE/PSV IDs assigned.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 29th {ACM} {International} {Conference} on {Architectural} {Support} for {Programming} {Languages} and {Operating} {Systems}, {Volume} 4},
	publisher = {ACM},
	author = {Ye, Chengfeng and Cai, Yuandao and Zhou, Anshunkang and Huang, Heqing and Ling, Hao and Zhang, Charles},
	month = apr,
	year = {2024},
	pages = {170--187},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\PQ4X5RGY\\Ye 等 - 2024 - Manta Hybrid-Sensitive Type Inference Toward Type-Assisted Bug Detection for Stripped Binaries.pdf:application/pdf},
}

@inproceedings{shang_how_2024,
	address = {Flagstaff, AZ, USA},
	title = {How {Far} {Have} {We} {Gone} in {Binary} {Code} {Understanding} {Using} {Large} {Language} {Models}},
	copyright = {https://doi.org/10.15223/policy-029},
	url = {https://ieeexplore.ieee.org/document/10795058/},
	doi = {10.1109/icsme58944.2024.00012},
	abstract = {Binary code analysis plays a pivotal role in various software security applications, such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, understanding binary code is challenging for reverse engineers due to the absence of semantic information. Therefore, automated tools are needed to assist human players in interpreting binary code. In recent years, two groups of technologies have shown promising prospects: (1) Deep learning-based technologies have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This makes participants wonder about the ability of LLMs in binary code understanding.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {2024 {IEEE} {International} {Conference} on {Software} {Maintenance} and {Evolution} ({ICSME})},
	publisher = {IEEE},
	author = {Shang, Xiuwei and Cheng, Shaoyin and Chen, Guoqiang and Zhang, Yanming and Hu, Li and Yu, Xiao and Li, Gangyang and Zhang, Weiming and Yu, Nenghai},
	month = oct,
	year = {2024},
	pages = {1--12},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\A79EBYJB\\Shang 等 - 2024 - How Far Have We Gone in Binary Code Understanding Using Large Language Models.pdf:application/pdf},
}

@inproceedings{song_typefsl_2024,
	address = {Sacramento CA USA},
	title = {{TypeFSL}: {Type} {Prediction} from {Binaries} via {Inter}-procedural {Data}-flow {Analysis} and {Few}-shot {Learning}},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	shorttitle = {{TypeFSL}},
	url = {https://dl.acm.org/doi/10.1145/3691620.3695502},
	doi = {10.1145/3691620.3695502},
	abstract = {Type recovery in stripped binaries is a critical and challenging task in reverse engineering, as it is the basis for many security applications (e.g., vulnerability detection). Traditional analysis methods are limited by software complexity and emerging types in realworld projects. To address these limitations, machine learning methods have been explored. However, the existing supervised learning approaches struggle with analyzing complicated and uncommon types due to the limited availability of samples. Additionally, none of the existing works can capture ﬁne-grained and inter-procedural features in the binaries. In this paper, we present TypeFSL, a framework that addresses the challenge of imbalanced type distributions by incorporating few-shot learning and captures inter-procedural semantics through program slicing. Moreover, based on a dataset with 3,003,117 functions, TypeFSL achieves an average of 77.9\% and 84.6\% accuracy across all architecture and optimizations in 20way 5-shot and 10-shot classiﬁcation tasks. Our prototype outperforms existing techniques in prediction accuracy and obfuscation resistance. Finally, the case studies demonstrate how TypeFSL predicts uncommon and complicated types in practical analysis.},
	language = {en},
	urldate = {2025-07-12},
	booktitle = {Proceedings of the 39th {IEEE}/{ACM} {International} {Conference} on {Automated} {Software} {Engineering}},
	publisher = {ACM},
	author = {Song, Zirui and Zhou, YuTong and Dong, Shuaike and Zhang, Ke and Zhang, Kehuan},
	month = oct,
	year = {2024},
	pages = {1269--1281},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\FX4LLH4S\\Song 等 - 2024 - TypeFSL Type Prediction from Binaries via Inter-procedural Data-flow Analysis and Few-shot Learning.pdf:application/pdf},
}

@article{FoC,
	title = {{FoC}: {Figure} out the {Cryptographic} {Functions} in {Stripped} {Binaries} with {LLMs}},
	issn = {1049-331X, 1557-7392},
	shorttitle = {{FoC}},
	url = {https://dl.acm.org/doi/10.1145/3731449},
	doi = {10.1145/3731449},
	abstract = {Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task, which is crucial in software security fields such as malware analysis and legacy code inspection. However, the inherent high logical complexity of cryptographic algorithms makes their analysis more difficult than that of ordinary code, and the general absence of symbolic information in binaries exacerbates this challenge. Existing methods for cryptographic algorithm identification frequently rely on data or structural pattern matching, which limits their generality and effectiveness while requiring substantial manual effort. In response to these challenges, we present
              FoC
              (
              
                F
              
              igure
              
                o
              
              ut the
              
                C
              
              ryptographic functions), a novel framework that leverages large language models (LLMs) to identify and analyze cryptographic functions in stripped binaries.
            
            
              In FoC, we first build an LLM-based generative model (
              FoC-BinLLM
              ) to summarize the semantics of cryptographic functions in natural language form, which is intuitively readable to analysts. Subsequently, based on the semantic insights provided by FoC-BinLLM, we further develop a binary code similarity detection model (
              FoC-Sim
              ), which allows analysts to effectively retrieve similar implementations of unknown cryptographic functions from a library of known cryptographic functions. The predictions of generative model like FoC-BinLLM are inherently difficult to reflect minor alterations in binary code, such as those introduced by vulnerability patches. In contrast, the change-sensitive representations generated by FoC-Sim compensate for the shortcomings to some extent. To support the development and evaluation of these models, and to facilitate further research in this domain, we also construct a comprehensive cryptographic binary dataset and introduce an automatic method to create semantic labels for extensive binary functions. Our evaluation results are promising. FoC-BinLLM outperforms ChatGPT by 14.61\% on the ROUGE-L score, demonstrating superior capability in summarizing the semantics of cryptographic functions. FoC-Sim also surpasses previous best methods with a 52\% higher Recall@1 in retrieving similar cryptographic functions. Beyond these metrics, our method has proven its practical utility in real-world scenarios, including cryptographic-related virus analysis and 1-day vulnerability detection.},
	language = {en},
	urldate = {2025-09-02},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Shang, Xiuwei and Chen, Guoqiang and Cheng, Shaoyin and Guo, Shikai and Zhang, Yanming and Zhang, Weiming and Yu, Nenghai},
	month = apr,
	year = {2025},
	pages = {3731449},
	file = {已提交版本:D\:\\tools\\Zotero\\Volume\\storage\\Q7IMG8MT\\Shang 等 - 2025 - FoC Figure out the Cryptographic Functions in Stripped Binaries with LLMs.pdf:application/pdf},
}

@misc{BinMetric,
	title = {{BinMetric}: {A} {Comprehensive} {Binary} {Analysis} {Benchmark} for {Large} {Language} {Models}},
	shorttitle = {{BinMetric}},
	url = {http://arxiv.org/abs/2505.07360},
	doi = {10.48550/arXiv.2505.07360},
	abstract = {Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.},
	urldate = {2025-09-02},
	publisher = {arXiv},
	author = {Shang, Xiuwei and Chen, Guoqiang and Cheng, Shaoyin and Wu, Benlong and Hu, Li and Li, Gangyang and Zhang, Weiming and Yu, Nenghai},
	month = may,
	year = {2025},
	note = {arXiv:2505.07360 [cs]},
	keywords = {Computer Science - Software Engineering},
	annote = {Comment: 23 pages, 5 figures, to be published in IJCAI 2025},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\8CP9W3NB\\Shang 等 - 2025 - BinMetric A Comprehensive Binary Analysis Benchmark for Large Language Models.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\IJALABHW\\2505.html:text/html},
}

@article{Bin2Summary,
	title = {{Bin2Summary}: {Beyond} {Function} {Name} {Prediction} in {Stripped} {Binaries} with {Functionality}-{Specific} {Code} {Embeddings}},
	volume = {1},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	issn = {2994-970X},
	shorttitle = {{Bin2Summary}},
	url = {https://dl.acm.org/doi/10.1145/3643729},
	doi = {10.1145/3643729},
	abstract = {Nowadays, closed-source software only with stripped binaries still dominates the ecosystem, which brings obstacles to understanding the functionalities of the software and further conducting the security analysis. With such an urgent need, research has traditionally focused on predicting function names, which can only provide fragmented and abbreviated information about functionality.  To advance the state-of-the-art, this paper presents Bin2Summary to automatically summarize the functionality of the function in stripped binaries with natural language sentences. Specifically, the proposed framework includes a functionality-specific code embedding module to facilitate fine-grained similarity detection and an attention-based seq2seq model to generate summaries in natural language. Based on 16 widely-used projects (e.g., Coreutils), we have evaluated Bin2Summary with 38,167 functions, which are filtered from 162,406 functions, and all of them have a high-quality comment. Bin2Summary achieves 0.728 in precision and 0.729 in recall on our datasets, and the functionality-specific embedding module can improve the existing assembly language model by up to 109.5\% and 109.9\% in precision and recall. Meanwhile, the experiments demonstrated that Bin2Summary has outstanding transferability in analyzing the cross-architecture (i.e., in x64 and x86) and cross-environment (i.e., in Cygwin and MSYS2) binaries. Finally, the case study illustrates how Bin2Summary outperforms the existing works in providing functionality summaries with abundant semantics beyond function names.},
	language = {en},
	number = {FSE},
	urldate = {2025-09-02},
	journal = {Proceedings of the ACM on Software Engineering},
	author = {Song, Zirui and Chen, Jiongyi and Zhang, Kehuan},
	month = jul,
	year = {2024},
	pages = {47--69},
}

@misc{noauthor_lightweight_nodate,
	title = {A lightweight framework for function name reassignment based on large-scale stripped binaries {\textbar} {Proceedings} of the 30th {ACM} {SIGSOFT} {International} {Symposium} on {Software} {Testing} and {Analysis}},
	url = {https://dl.acm.org/doi/10.1145/3460319.3464804},
	language = {EN},
	urldate = {2025-09-02},
	journal = {ACM Conferences},
	doi = {10.1145/3460319.3464804},
	note = {Archive Location: world},
	file = {Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\BGR8AHKE\\3460319.html:text/html},
}

@inproceedings{Debin,
	address = {Vienna Austria},
	title = {Statistical {Deobfuscation} of {Android} {Applications}},
	isbn = {978-1-4503-4139-4},
	url = {https://dl.acm.org/doi/10.1145/2976749.2978422},
	doi = {10.1145/2976749.2978422},
	language = {en},
	urldate = {2025-09-07},
	booktitle = {Proceedings of the 2016 {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security}},
	publisher = {ACM},
	author = {Bichsel, Benjamin and Raychev, Veselin and Tsankov, Petar and Vechev, Martin},
	month = oct,
	year = {2016},
	pages = {343--355},
	annote = {Debin
},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\XRBV3XXT\\Bichsel 等 - 2016 - Statistical Deobfuscation of Android Applications.pdf:application/pdf},
}

@misc{CodeLLaMA,
	title = {Code {Llama}: {Open} {Foundation} {Models} for {Code}},
	shorttitle = {Code {Llama}},
	url = {http://arxiv.org/abs/2308.12950},
	doi = {10.48550/arXiv.2308.12950},
	abstract = {We release Code Llama, a family of large language models for code based on Llama 2 providing state-of-the-art performance among open models, infilling capabilities, support for large input contexts, and zero-shot instruction following ability for programming tasks. We provide multiple flavors to cover a wide range of applications: foundation models (Code Llama), Python specializations (Code Llama - Python), and instruction-following models (Code Llama - Instruct) with 7B, 13B, 34B and 70B parameters each. All models are trained on sequences of 16k tokens and show improvements on inputs with up to 100k tokens. 7B, 13B and 70B Code Llama and Code Llama - Instruct variants support infilling based on surrounding content. Code Llama reaches state-of-the-art performance among open models on several code benchmarks, with scores of up to 67\% and 65\% on HumanEval and MBPP, respectively. Notably, Code Llama - Python 7B outperforms Llama 2 70B on HumanEval and MBPP, and all our models outperform every other publicly available model on MultiPL-E. We release Code Llama under a permissive license that allows for both research and commercial use.},
	urldate = {2025-09-07},
	publisher = {arXiv},
	author = {Rozière, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and Rapin, Jérémy and Kozhevnikov, Artyom and Evtimov, Ivan and Bitton, Joanna and Bhatt, Manish and Ferrer, Cristian Canton and Grattafiori, Aaron and Xiong, Wenhan and Défossez, Alexandre and Copet, Jade and Azhar, Faisal and Touvron, Hugo and Martin, Louis and Usunier, Nicolas and Scialom, Thomas and Synnaeve, Gabriel},
	month = jan,
	year = {2024},
	note = {arXiv:2308.12950 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\GP2SR48I\\Rozière 等 - 2024 - Code Llama Open Foundation Models for Code.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\PPL9KBWT\\2308.html:text/html},
}

@inproceedings{sig_meet_opt,
	title = {When {Function} {Signature} {Recovery} {Meets} {Compiler} {Optimization}},
	url = {https://ieeexplore.ieee.org/document/9519479},
	doi = {10.1109/SP40001.2021.00006},
	abstract = {Matching indirect function callees and callers using function signatures recovered from binary executables (number of arguments and argument types) has been proposed to construct a more fine-grained control-flow graph (CFG) to help control-flow integrity (CFI) enforcement. However, various compiler optimizations may violate calling conventions and result in unmatched function signatures. In this paper, we present eight scenarios in which compiler optimizations impact function signature recovery, and report experimental results with 1,344 real-world applications of various optimization levels. Most interestingly, our experiments show that compiler optimizations have both positive and negative impacts on function signature recovery, e.g., its elimination of redundant instructions at callers makes counting of the number of arguments more accurate, while it hurts argument type matching as the compiler chooses the most efficient (but potentially different) types at callees and callers. To better deal with these compiler optimizations, we propose a set of improved policies and report our more accurate CFG models constructed from the 1,344 applications. We additionally compare our results recovered from binary executables with those extracted from program source and reveal scenarios where compiler optimization makes the task of accurate function signature recovery undecidable.},
	urldate = {2025-09-07},
	booktitle = {2021 {IEEE} {Symposium} on {Security} and {Privacy} ({SP})},
	author = {Lin, Yan and Gao, Debin},
	month = may,
	year = {2021},
	note = {ISSN: 2375-1207},
	keywords = {Optimization, Privacy, Program processors, Security, Task analysis},
	pages = {36--52},
	annote = {sig\_meet\_opt
},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\8QU648ZR\\Lin和Gao - 2021 - When Function Signature Recovery Meets Compiler Optimization.pdf:application/pdf},
}

@misc{LLaMA,
	title = {{LLaMA}: {Open} and {Efficient} {Foundation} {Language} {Models}},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2025-09-07},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\BUFU8ALC\\Touvron 等 - 2023 - LLaMA Open and Efficient Foundation Language Models.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\GWIQLTDA\\2302.html:text/html},
}

@misc{Ghidra,
	title = {Ghidra},
	url = {http://ghidra.net/},
	urldate = {2025-09-07},
	journal = {Ghidra},
	author = {National Security Agency},
	file = {Ghidra:D\:\\tools\\Zotero\\Volume\\storage\\7VA3U9MZ\\ghidra.net.html:text/html},
}

@article{CORE,
	title = {{CORE}: {Resolving} {Code} {Quality} {Issues} using {LLMs}},
	volume = {1},
	copyright = {https://creativecommons.org/licenses/by/4.0/},
	issn = {2994-970X},
	shorttitle = {{CORE}},
	url = {https://dl.acm.org/doi/10.1145/3643762},
	doi = {10.1145/3643762},
	abstract = {As software projects progress, quality of code assumes paramount importance as it affects reliability, maintainability and security of software. For this reason, static analysis tools are used in developer workflows to flag code quality issues. However, developers need to spend extra efforts to revise their code to improve code quality based on the tool findings. In this work, we investigate the use of (instruction-following) large language models (LLMs) to assist developers in revising code to resolve code quality issues.    We present a tool, CORE (short for COde REvisions), architected using a pair of LLMs organized as a duo comprised of a proposer and a ranker. Providers of static analysis tools recommend ways to mitigate the tool warnings and developers follow them to revise their code. The proposer LLM of CORE takes the same set of recommendations and applies them to generate candidate code revisions. The candidates which pass the static quality checks are retained. However, the LLM may introduce subtle, unintended functionality changes which may go un-detected by the static analysis. The ranker LLM evaluates the changes made by the proposer using a rubric that closely follows the acceptance criteria that a developer would enforce. CORE uses the scores assigned by the ranker LLM to rank the candidate revisions before presenting them to the developer.    We conduct a variety of experiments on two public benchmarks to show the ability of CORE:  (1) to generate code revisions acceptable to both static analysis tools and human reviewers (the latter evaluated with user study on a subset of the Python benchmark),  (2) to reduce human review efforts by detecting and eliminating revisions with unintended changes,  (3) to readily work across multiple languages (Python and Java), static analysis tools (CodeQL and SonarQube) and quality checks (52 and 10 checks, respectively),  and  (4) to achieve fix rate comparable to a rule-based automated program repair tool but with much smaller engineering efforts (on the Java benchmark).  CORE could revise 59.2\% Python files (across 52 quality checks) so that they pass scrutiny by both a tool and a human reviewer. The ranker LLM reduced false positives by 25.8\% in these cases. CORE produced revisions that passed the static analysis tool in 76.8\% Java files (across 10 quality checks) comparable to 78.3\% of a specialized program repair tool, with significantly much less engineering efforts. We release code, data, and supplementary material publicly at http://aka.ms/COREMSRI.},
	language = {en},
	number = {FSE},
	urldate = {2025-09-07},
	journal = {Proceedings of the ACM on Software Engineering},
	author = {Wadhwa, Nalin and Pradhan, Jui and Sonwane, Atharv and Sahu, Surya Prakash and Natarajan, Nagarajan and Kanade, Aditya and Parthasarathy, Suresh and Rajamani, Sriram},
	month = jul,
	year = {2024},
	pages = {789--811},
	annote = {CORE
},
}

@article{SimLLM,
	title = {{SimLLM}: {Calculating} {Semantic} {Similarity} in {Code} {Summaries} using a {Large} {Language} {Model}-{Based} {Approach}},
	volume = {1},
	copyright = {https://creativecommons.org/licenses/by-nc/4.0/},
	issn = {2994-970X},
	shorttitle = {{SimLLM}},
	url = {https://dl.acm.org/doi/10.1145/3660769},
	doi = {10.1145/3660769},
	abstract = {Code summaries are pivotal in software engineering, serving to improve code readability, maintainability, and collaboration. While recent advancements in Large Language Models (LLMs) have opened new avenues for automatic code summarization, existing metrics for evaluating summary quality, such as BLEU and BERTScore, have notable limitations. Specifically, these existing metrics either fail to capture the nuances of semantic meaning in summaries or are further limited in understanding domain-specific terminologies and expressions prevalent in code summaries. In this paper, we present SimLLM, a novel LLM-based approach designed to more precisely evaluate the semantic similarity of code summaries. Built upon an autoregressive LLM using a specialized pretraining task on permutated inputs and a pooling-based pairwise similarity measure, SimLLM overcomes the shortcomings of existing metrics. Our empirical evaluations demonstrate that SimLLM not only outperforms existing metrics but also shows a significantly high correlation with human ratings.},
	language = {en},
	number = {FSE},
	urldate = {2025-09-07},
	journal = {Proceedings of the ACM on Software Engineering},
	author = {Jin, Xin and Lin, Zhiqiang},
	month = jul,
	year = {2024},
	pages = {1376--1399},
	annote = {SimLLM
},
}

@misc{Calibration_LLM_cs,
	title = {Calibration of {Large} {Language} {Models} on {Code} {Summarization}},
	url = {http://arxiv.org/abs/2404.19318},
	doi = {10.48550/arXiv.2404.19318},
	abstract = {A brief, fluent, and relevant summary can be helpful during program comprehension; however, such a summary does require significant human effort to produce. Often, good summaries are unavailable in software projects, which makes maintenance more difficult. There has been a considerable body of research into automated AI-based methods, using Large Language models (LLMs), to generate summaries of code; there also has been quite a bit of work on ways to measure the performance of such summarization methods, with special attention paid to how closely these AI-generated summaries resemble a summary a human might have produced. Measures such as BERTScore and BLEU have been suggested and evaluated with human-subject studies. However, LLM-generated summaries can be inaccurate, incomplete, etc.: generally, too dissimilar to one that a good developer might write. Given an LLM-generated code summary, how can a user rationally judge if a summary is sufficiently good and reliable? Given just some input source code, and an LLM-generated summary, existing approaches can help judge brevity, fluency and relevance of the summary; however, it's difficult to gauge whether an LLM-generated summary sufficiently resembles what a human might produce, without a "golden" human-produced summary to compare against. We study this resemblance question as calibration problem: given just the code \& the summary from an LLM, can we compute a confidence measure, that provides a reliable indication of whether the summary sufficiently resembles what a human would have produced in this situation? We examine this question using several LLMs, for several languages, and in several different settings. Our investigation suggests approaches to provide reliable predictions of the likelihood that an LLM-generated summary would sufficiently resemble a summary a human might write for the same code.},
	urldate = {2025-09-07},
	publisher = {arXiv},
	author = {Virk, Yuvraj and Devanbu, Premkumar and Ahmed, Toufique},
	month = may,
	year = {2025},
	note = {arXiv:2404.19318 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Software Engineering},
	annote = {Calibration\_LLM\_cs
},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\8W32NFVA\\Virk 等 - 2025 - Calibration of Large Language Models on Code Summarization.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\SR96VHBE\\2404.html:text/html},
}

@inproceedings{DeGPT,
	address = {San Diego, CA, USA},
	title = {{DeGPT}: {Optimizing} {Decompiler} {Output} with {LLM}},
	isbn = {978-1-891562-93-8},
	shorttitle = {{DeGPT}},
	url = {https://www.ndss-symposium.org/wp-content/uploads/2024-401-paper.pdf},
	doi = {10.14722/ndss.2024.24401},
	abstract = {Reverse engineering is essential in malware analysis, vulnerability discovery, etc. Decompilers assist the reverse engineers by lifting the assembly to the high-level programming language, which highly boosts binary comprehension. However, decompilers suffer from problems such as meaningless variable names, redundant variables, and lacking comments describing the purpose of the code. Previous studies have shown promising performance in refining the decompiler output by training the models with huge datasets containing various decompiler outputs. However, even datasets that take much time to construct cover limited binaries in the real world. The performance degrades severely facing the binary migration.},
	language = {en},
	urldate = {2025-09-07},
	booktitle = {Proceedings 2024 {Network} and {Distributed} {System} {Security} {Symposium}},
	publisher = {Internet Society},
	author = {Hu, Peiwei and Liang, Ruigang and Chen, Kai},
	year = {2024},
	file = {PDF:D\:\\tools\\Zotero\\Volume\\storage\\BPZ8W6AX\\Hu 等 - 2024 - DeGPT Optimizing Decompiler Output with LLM.pdf:application/pdf},
}




@misc{BinaryLLM-Eval,
	title = {An {Empirical} {Study} on the {Effectiveness} of {Large} {Language} {Models} for {Binary} {Code} {Understanding}},
	url = {http://arxiv.org/abs/2504.21803},
	doi = {10.48550/arXiv.2504.21803},
	abstract = {Binary code analysis plays a pivotal role in the field of software security and is widely used in tasks such as software maintenance, malware detection, software vulnerability discovery, patch analysis, etc. However, unlike source code, reverse engineers face significant challenges in understanding binary code due to the lack of intuitive semantic information. Although traditional reverse tools can convert binary code into C-like pseudo code, the lack of code comments and symbolic information such as function names still makes code understanding difficult. In recent years, two groups of techniques have shown promising prospects: (1) Deep learning-based techniques have demonstrated competitive results in tasks related to binary code understanding, furthermore, (2) Large Language Models (LLMs) have been extensively pre-trained at the source-code level for tasks such as code understanding and generation. This has left participants wondering about the capabilities of LLMs in binary code understanding. To this end, this work proposes a benchmark to evaluate the effectiveness of LLMs in real-world reverse engineering scenarios, which covers two key binary code understanding tasks, i.e., function name recovery and binary code summarization. To more comprehensively evaluate, we include binaries with multiple target architectures as well as different optimization options. We gain valuable insights into the capabilities and limitations through extensive empirical studies of popular LLMs using our benchmark. Our evaluations reveal that existing LLMs can understand binary code to a certain extent, thereby improving the efficiency of binary code analysis. Our results highlight the great potential of the LLMs in advancing the field of binary code understanding, and provide new directions for binary code analysis techniques.},
	urldate = {2025-09-08},
	publisher = {arXiv},
	author = {Shang, Xiuwei and Fu, Zhenkan and Cheng, Shaoyin and Chen, Guoqiang and Li, Gangyang and Hu, Li and Zhang, Weiming and Yu, Nenghai},
	month = apr,
	year = {2025},
	note = {arXiv:2504.21803 [cs]},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Software Engineering},
	annote = {BinaryLLM-Eval
},
	annote = {Comment: 38 pages, 9 figures},
	file = {Full Text PDF:D\:\\tools\\Zotero\\Volume\\storage\\3KC8XFJQ\\Shang 等 - 2025 - An Empirical Study on the Effectiveness of Large Language Models for Binary Code Understanding.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\AJ6LCMWT\\2504.html:text/html},
}



@misc{Deepseek-R1,
	title = {{DeepSeek}-{R1}: {Incentivizing} {Reasoning} {Capability} in {LLMs} via {Reinforcement} {Learning}},
	shorttitle = {{DeepSeek}-{R1}},
	url = {http://arxiv.org/abs/2501.12948},
	doi = {10.48550/arXiv.2501.12948},
	abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
	urldate = {2025-09-08},
	publisher = {arXiv},
	author = {DeepSeek-AI and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
	month = jan,
	year = {2025},
	note = {arXiv:2501.12948 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	annote = {Deepseek-R1
},
	file = {Preprint PDF:D\:\\tools\\Zotero\\Volume\\storage\\D4JCAMX2\\DeepSeek-AI 等 - 2025 - DeepSeek-R1 Incentivizing Reasoning Capability in LLMs via Reinforcement Learning.pdf:application/pdf;Snapshot:D\:\\tools\\Zotero\\Volume\\storage\\CN8MWHTY\\2501.html:text/html},
}
